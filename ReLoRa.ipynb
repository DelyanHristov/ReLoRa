{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b6f54c-c279-49bd-b102-e9cec36d5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d01b703-f775-4bb7-9cae-da915253eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric,concatenate_datasets\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments,AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd8039b-fac5-49c8-bbc6-4f025701517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_requires_grad(model):\n",
    "    print(\"Checking which high-level components require gradients:\\n\")\n",
    "\n",
    "    # Checking Embeddings\n",
    "    print(\"Embeddings:\")\n",
    "    for name, param in model.wrapped_model.bert.embeddings.named_parameters():\n",
    "        grad_status = \"requires gradient\" if param.requires_grad else \"does NOT require gradient\"\n",
    "        print(f\"  {name} - {grad_status}\")\n",
    "\n",
    "    # Checking Encoder\n",
    "    print(\"\\nEncoder:\")\n",
    "    for i, layer in enumerate(model.wrapped_model.bert.encoder.layer):\n",
    "        print(f\"  Layer {i}:\")\n",
    "        for name, param in layer.named_parameters():\n",
    "            grad_status = \"requires gradient\" if param.requires_grad else \"does NOT require gradient\"\n",
    "            print(f\"    {name} - {grad_status}\")\n",
    "\n",
    "    # Checking Attention\n",
    "    print(\"\\nAttention:\")\n",
    "    for i, layer in enumerate(model.wrapped_model.bert.encoder.layer):\n",
    "        print(f\"  Layer {i} Attention:\")\n",
    "        for name, param in layer.attention.named_parameters():\n",
    "            grad_status = \"requires gradient\" if param.requires_grad else \"does NOT require gradient\"\n",
    "            print(f\"    {name} - {grad_status}\")\n",
    "\n",
    "    # Checking MLM Head\n",
    "    print(\"\\nMasked LM Head:\")\n",
    "    for name, param in model.wrapped_model.cls.named_parameters():\n",
    "        grad_status = \"requires gradient\" if param.requires_grad else \"does NOT require gradient\"\n",
    "        print(f\"  {name} - {grad_status}\")\n",
    "\n",
    "    print(\"\\nCheck complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa8b37-9ee2-4bdc-bede-e82a1baafcb2",
   "metadata": {},
   "source": [
    "test the relora experiments---------\n",
    "test trainable scaling\n",
    "\n",
    "\n",
    "\n",
    "test bigger bert\n",
    "maybe there is a mistake in the training loop, fix it ----------\n",
    "try warmup or bigger model\n",
    "try using not mlm but normal causal language modeling (change the model initialization + collator etc)\n",
    "the form_pretrian methosd may be wrong but i don't use it rn.\n",
    "check the dataloader\n",
    "check in the original reelora file if lora_a is 0--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a98ab1-dd4b-4892-a0bb-124f5042d8c3",
   "metadata": {},
   "source": [
    "problems:\n",
    "less time for training\n",
    "higher perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23182e71-c30e-46be-a842-94e7f2d21dae",
   "metadata": {},
   "source": [
    "check hyperparameters\n",
    "compare with lora training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42f9800-416e-419d-acc5-ab403ac56a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-21 22:49:33.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:33.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240921_224934-9c92utk5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgraceful-snowball-734\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/9c92utk5\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_name_or_path             bert_c4\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdataset_path                   /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mbatch_size                     64\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtotal_batch_size               64\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_r                         64\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_alpha                     64.0\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrelora                         15625\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mreset_optimizer_on_relora      False\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.9\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlr                             0.001\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mscheduler                      cosine_restarts\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcycle_length                   15625\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrestart_warmup_steps           500\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmin_lr_ratio                   0.1\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mweight_decay                   0.02\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmup_steps                   9000\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1meval_every                     3500\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mnum_training_steps             62500\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_every                     600000\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_dir                       checkpoint/graceful-snowball-734\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtags                           ['relora_bert_medium']\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrun_name                       graceful-snowball-734\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m473\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:35.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1mUsing HuggingFace model bert_c4 revision None\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m565\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 36685626 || all params: 42977082 || trainable%: 85.36\n",
      "\u001b[32m2024-09-21 22:49:37.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m630\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): BertForMaskedLM(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 512)\n",
      "        (token_type_embeddings): Embedding(1, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-7): 8 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (key): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (value): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mTotal params  before LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m632\u001b[0m - \u001b[1mTotal params  after  LoRA: 42.98M\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mTrainable params: 36.69M\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mIn total, added 1.57M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mSaving model to checkpoint/graceful-snowball-734 every 600000 update steps\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m657\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-09-21 22:49:37.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[1mScheduler will run for 62500 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-21 22:49:37.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1mFull training set size: 4000000\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m763\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mTrain set size after shard: 4000000\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-21 22:49:37.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m772\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-09-21 22:49:37.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m871\u001b[0m - \u001b[1mStarting training at update step 0 with 62500 update steps\u001b[0m\n",
      "Update steps:   0%|                                   | 0/62500 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-21 22:49:54.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m904\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Update steps:   6%|‚ñà‚ñè                    | 3500/62500 [06:39<1:46:11,  9.26it/s]\u001b[32m2024-09-21 22:56:16.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 3500\u001b[0m\n",
      "\u001b[32m2024-09-21 22:56:30.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10204864.0 tokens, eval loss: 6.5037\u001b[0m\n",
      "\u001b[32m2024-09-21 22:56:30.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 667.58\u001b[0m\n",
      "\u001b[32m2024-09-21 22:56:30.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.01 seconds\u001b[0m\n",
      "\u001b[32m2024-09-21 22:56:30.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 3500: 6.503652572631836, Perplexity: 667.5755534991663\u001b[0m\n",
      "Update steps:  11%|‚ñà‚ñà‚ñç                   | 7000/62500 [13:16<1:35:21,  9.70it/s]\u001b[32m2024-09-21 23:02:54.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 7000\u001b[0m\n",
      "\u001b[32m2024-09-21 23:03:06.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8358464.0 tokens, eval loss: 6.1272\u001b[0m\n",
      "\u001b[32m2024-09-21 23:03:06.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 458.15\u001b[0m\n",
      "\u001b[32m2024-09-21 23:03:06.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.45 seconds\u001b[0m\n",
      "\u001b[32m2024-09-21 23:03:06.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 7000: 6.127203464508057, Perplexity: 458.1531260124792\u001b[0m\n",
      "Update steps:  17%|‚ñà‚ñà‚ñà‚ñå                 | 10500/62500 [19:52<1:38:23,  8.81it/s]\u001b[32m2024-09-21 23:09:30.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 10500\u001b[0m\n",
      "\u001b[32m2024-09-21 23:09:45.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10905216.0 tokens, eval loss: 4.3984\u001b[0m\n",
      "\u001b[32m2024-09-21 23:09:45.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 81.32\u001b[0m\n",
      "\u001b[32m2024-09-21 23:09:45.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.99 seconds\u001b[0m\n",
      "\u001b[32m2024-09-21 23:09:45.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 10500: 4.398418426513672, Perplexity: 81.32214994653648\u001b[0m\n",
      "Update steps:  22%|‚ñà‚ñà‚ñà‚ñà‚ñã                | 14000/62500 [26:31<1:27:00,  9.29it/s]\u001b[32m2024-09-21 23:16:08.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 14000\u001b[0m\n",
      "\u001b[32m2024-09-21 23:16:20.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8114240.0 tokens, eval loss: 3.8506\u001b[0m\n",
      "\u001b[32m2024-09-21 23:16:20.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 47.02\u001b[0m\n",
      "\u001b[32m2024-09-21 23:16:20.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.18 seconds\u001b[0m\n",
      "\u001b[32m2024-09-21 23:16:20.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 14000: 3.8506433963775635, Perplexity: 47.0233081269178\u001b[0m\n",
      "Update steps:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 15626/62500 [29:40<1:30:21,  8.65it/s]\u001b[32m2024-09-21 23:19:18.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1015\u001b[0m - \u001b[1margs.resume_from=None, local_step=15626, args.relora=15625, thresh: 15626\u001b[0m\n",
      "\u001b[32m2024-09-21 23:19:18.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1016\u001b[0m - \u001b[1mPerforming lora reset at update step 15626. Current lr is 1.9223699443912782e-06\u001b[0m\n",
      "\u001b[32m2024-09-21 23:19:18.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1027\u001b[0m - \u001b[1mLoRA reset took 0.00s\u001b[0m\n",
      "\u001b[32m2024-09-21 23:19:18.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1036\u001b[0m - \u001b[1mPerforming optimizer reset at update step 15626. Current lr is 1.9223699443912782e-06\u001b[0m\n",
      "\u001b[32m2024-09-21 23:19:18.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-09-21 23:19:18.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.09\u001b[0m\n",
      "Update steps:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 15627/62500 [29:40<1:32:29,  8.45it/s]\u001b[32m2024-09-21 23:19:18.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1050\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 3.8447398887825564e-06\u001b[0m\n",
      "Update steps:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 17500/62500 [33:06<1:20:48,  9.28it/s]\u001b[32m2024-09-21 23:22:44.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 17500\u001b[0m\n",
      "\u001b[32m2024-09-21 23:22:57.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10009984.0 tokens, eval loss: 3.5981\u001b[0m\n",
      "\u001b[32m2024-09-21 23:22:57.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 36.53\u001b[0m\n",
      "\u001b[32m2024-09-21 23:22:57.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.77 seconds\u001b[0m\n",
      "\u001b[32m2024-09-21 23:22:57.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 17500: 3.5981199741363525, Perplexity: 36.52949345402633\u001b[0m\n",
      "Update steps:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 21000/62500 [39:44<1:13:41,  9.39it/s]\u001b[32m2024-09-21 23:29:22.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 21000\u001b[0m\n",
      "\u001b[32m2024-09-21 23:29:33.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8103104.0 tokens, eval loss: 3.4181\u001b[0m\n",
      "\u001b[32m2024-09-21 23:29:33.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 30.51\u001b[0m\n",
      "\u001b[32m2024-09-21 23:29:33.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.24 seconds\u001b[0m\n",
      "\u001b[32m2024-09-21 23:29:33.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 21000: 3.4180963039398193, Perplexity: 30.51127550365613\u001b[0m\n",
      "Update steps:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 24500/62500 [46:21<1:08:06,  9.30it/s]\u001b[32m2024-09-21 23:35:59.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 24500\u001b[0m\n",
      "\u001b[32m2024-09-21 23:36:14.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11018304.0 tokens, eval loss: 3.3037\u001b[0m\n",
      "\u001b[32m2024-09-21 23:36:14.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 27.21\u001b[0m\n",
      "\u001b[32m2024-09-21 23:36:14.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.14 seconds\u001b[0m\n",
      "\u001b[32m2024-09-21 23:36:14.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 24500: 3.303731918334961, Perplexity: 27.21401011180701\u001b[0m\n",
      "Update steps:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 28000/62500 [53:02<1:02:56,  9.14it/s]\u001b[32m2024-09-21 23:42:40.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 28000\u001b[0m\n",
      "\u001b[32m2024-09-21 23:42:55.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11167232.0 tokens, eval loss: 3.2195\u001b[0m\n",
      "\u001b[32m2024-09-21 23:42:55.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.01\u001b[0m\n",
      "\u001b[32m2024-09-21 23:42:55.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.40 seconds\u001b[0m\n",
      "\u001b[32m2024-09-21 23:42:55.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 28000: 3.219451904296875, Perplexity: 25.014406134857413\u001b[0m\n",
      "Update steps:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 31250/62500 [59:14<55:14,  9.43it/s]\u001b[32m2024-09-21 23:48:52.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1015\u001b[0m - \u001b[1margs.resume_from=None, local_step=31251, args.relora=15625, thresh: 31251\u001b[0m\n",
      "\u001b[32m2024-09-21 23:48:52.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1016\u001b[0m - \u001b[1mPerforming lora reset at update step 31251. Current lr is 1.3094584085039247e-06\u001b[0m\n",
      "\u001b[32m2024-09-21 23:48:52.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1027\u001b[0m - \u001b[1mLoRA reset took 0.00s\u001b[0m\n",
      "\u001b[32m2024-09-21 23:48:52.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1036\u001b[0m - \u001b[1mPerforming optimizer reset at update step 31251. Current lr is 1.3094584085039247e-06\u001b[0m\n",
      "\u001b[32m2024-09-21 23:48:52.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-09-21 23:48:52.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.08\u001b[0m\n",
      "Update steps:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 31252/62500 [59:14<54:11,  9.61it/s]\u001b[32m2024-09-21 23:48:52.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1050\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 2.6189168170078494e-06\u001b[0m\n",
      "Update steps:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 31500/62500 [59:41<53:24,  9.67it/s]\u001b[32m2024-09-21 23:49:19.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 31500\u001b[0m\n",
      "\u001b[32m2024-09-21 23:49:34.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10895040.0 tokens, eval loss: 3.1372\u001b[0m\n",
      "\u001b[32m2024-09-21 23:49:34.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 23.04\u001b[0m\n",
      "\u001b[32m2024-09-21 23:49:34.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.96 seconds\u001b[0m\n",
      "\u001b[32m2024-09-21 23:49:34.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 31500: 3.137173891067505, Perplexity: 23.038664991404072\u001b[0m\n",
      "Update steps:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 35000/62500 [1:06:22<52:39,  8.70it/s]\u001b[32m2024-09-21 23:55:59.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 35000\u001b[0m\n",
      "\u001b[32m2024-09-21 23:56:14.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10333248.0 tokens, eval loss: 3.0784\u001b[0m\n",
      "\u001b[32m2024-09-21 23:56:14.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 21.72\u001b[0m\n",
      "\u001b[32m2024-09-21 23:56:14.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.21 seconds\u001b[0m\n",
      "\u001b[32m2024-09-21 23:56:14.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 35000: 3.0783629417419434, Perplexity: 21.722811763790066\u001b[0m\n",
      "Update steps:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 38500/62500 [1:13:01<47:54,  8.35it/s]\u001b[32m2024-09-22 00:02:38.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 38500\u001b[0m\n",
      "\u001b[32m2024-09-22 00:02:54.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11246208.0 tokens, eval loss: 3.0438\u001b[0m\n",
      "\u001b[32m2024-09-22 00:02:54.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 20.98\u001b[0m\n",
      "\u001b[32m2024-09-22 00:02:54.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.47 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 00:02:54.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 38500: 3.043794631958008, Perplexity: 20.984721639440146\u001b[0m\n",
      "Update steps:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 42000/62500 [1:19:41<36:57,  9.25it/s]\u001b[32m2024-09-22 00:09:18.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 42000\u001b[0m\n",
      "\u001b[32m2024-09-22 00:09:29.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8049536.0 tokens, eval loss: 2.9902\u001b[0m\n",
      "\u001b[32m2024-09-22 00:09:29.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 19.89\u001b[0m\n",
      "\u001b[32m2024-09-22 00:09:29.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.17 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 00:09:29.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 42000: 2.9902472496032715, Perplexity: 19.89059982655106\u001b[0m\n",
      "Update steps:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 45500/62500 [1:26:17<29:03,  9.75it/s]\u001b[32m2024-09-22 00:15:55.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 45500\u001b[0m\n",
      "\u001b[32m2024-09-22 00:16:10.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11015936.0 tokens, eval loss: 2.9851\u001b[0m\n",
      "\u001b[32m2024-09-22 00:16:10.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 19.79\u001b[0m\n",
      "\u001b[32m2024-09-22 00:16:10.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.18 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 00:16:10.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 45500: 2.98508358001709, Perplexity: 19.78815606167781\u001b[0m\n",
      "Update steps:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 46875/62500 [1:29:03<26:36,  9.79it/s]\u001b[32m2024-09-22 00:18:41.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1015\u001b[0m - \u001b[1margs.resume_from=None, local_step=46876, args.relora=15625, thresh: 46876\u001b[0m\n",
      "\u001b[32m2024-09-22 00:18:41.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1016\u001b[0m - \u001b[1mPerforming lora reset at update step 46876. Current lr is 5.322436819527499e-07\u001b[0m\n",
      "\u001b[32m2024-09-22 00:18:41.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1027\u001b[0m - \u001b[1mLoRA reset took 0.00s\u001b[0m\n",
      "\u001b[32m2024-09-22 00:18:41.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1036\u001b[0m - \u001b[1mPerforming optimizer reset at update step 46876. Current lr is 5.322436819527499e-07\u001b[0m\n",
      "\u001b[32m2024-09-22 00:18:41.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-09-22 00:18:41.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.05\u001b[0m\n",
      "Update steps:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 46877/62500 [1:29:03<26:38,  9.77it/s]\u001b[32m2024-09-22 00:18:41.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1050\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 1.0644873639054998e-06\u001b[0m\n",
      "Update steps:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 49000/62500 [1:32:57<27:34,  8.16it/s]\u001b[32m2024-09-22 00:22:35.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 49000\u001b[0m\n",
      "\u001b[32m2024-09-22 00:22:49.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10275584.0 tokens, eval loss: 2.9679\u001b[0m\n",
      "\u001b[32m2024-09-22 00:22:49.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 19.45\u001b[0m\n",
      "\u001b[32m2024-09-22 00:22:49.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.15 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 00:22:49.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 49000: 2.967946767807007, Perplexity: 19.451939217758838\u001b[0m\n",
      "Update steps:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 52500/62500 [1:39:37<17:48,  9.36it/s]\u001b[32m2024-09-22 00:29:15.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 52500\u001b[0m\n",
      "\u001b[32m2024-09-22 00:29:28.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10080960.0 tokens, eval loss: 2.9508\u001b[0m\n",
      "\u001b[32m2024-09-22 00:29:28.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 19.12\u001b[0m\n",
      "\u001b[32m2024-09-22 00:29:28.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.88 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 00:29:28.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 52500: 2.9507689476013184, Perplexity: 19.120650855458056\u001b[0m\n",
      "Update steps:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 56000/62500 [1:46:16<11:23,  9.52it/s]\u001b[32m2024-09-22 00:35:54.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 56000\u001b[0m\n",
      "\u001b[32m2024-09-22 00:36:08.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9750208.0 tokens, eval loss: 2.9545\u001b[0m\n",
      "\u001b[32m2024-09-22 00:36:08.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 19.19\u001b[0m\n",
      "\u001b[32m2024-09-22 00:36:08.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.43 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 00:36:08.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 56000: 2.954491138458252, Perplexity: 19.191954187231797\u001b[0m\n",
      "Update steps:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 59500/62500 [1:52:55<05:52,  8.51it/s]\u001b[32m2024-09-22 00:42:33.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 59500\u001b[0m\n",
      "\u001b[32m2024-09-22 00:42:47.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10289344.0 tokens, eval loss: 2.9524\u001b[0m\n",
      "\u001b[32m2024-09-22 00:42:47.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 19.15\u001b[0m\n",
      "\u001b[32m2024-09-22 00:42:47.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.28 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 00:42:47.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 59500: 2.9524405002593994, Perplexity: 19.152638757503\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:39<00:00,  9.18it/s]Warning: reached the end of the dataset. Training stopped, global_rank=0, update_step=62500\n",
      "\u001b[32m2024-09-22 00:48:17.617\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1081\u001b[0m - \u001b[33m\u001b[1mReached the end of the dataset. Training stopped\u001b[0m\n",
      "\u001b[32m2024-09-22 00:48:17.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1087\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:40<00:00,  8.78it/s]\n",
      "\u001b[32m2024-09-22 00:48:17.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1092\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/graceful-snowball-734/model_62500, update step 62500\u001b[0m\n",
      "\u001b[32m2024-09-22 00:48:17.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m243\u001b[0m - \u001b[1mSaving took 0.20 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 00:48:17.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1112\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:14.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 41435328.0 tokens, eval loss: 2.9512\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:14.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 19.13\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:14.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 56.91 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:14.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1131\u001b[0m - \u001b[1mFinal eval loss: 2.9511656761169434\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm ‚ñÅ‚ñÇ‚ñá‚ñÑ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ‚ñÑ‚ñá‚ñá‚ñÉ‚ñà‚ñÉ‚ñá‚ñÇ‚ñá‚ñá‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñà‚ñÇ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñá‚ñÑ‚ñÉ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 2.95117\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 41435328.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm 1.35156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 3.09375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity 19.12824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 9.19961\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 588.77496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 241397.73242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 1652867648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 62500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mgraceful-snowball-734\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/9c92utk5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240921_224934-9c92utk5/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "\u001b[32m2024-09-22 00:49:21.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1154\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
      "Rank 0 finished successfully\n"
     ]
    }
   ],
   "source": [
    "#chocolate-galaxy-701\n",
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path bert_c4 \\\n",
    "    --dataset_path /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM \\\n",
    "    --batch_size 64\\\n",
    "    --lr 1e-3\\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.02\\\n",
    "    --num_training_steps 62500 \\\n",
    "    --save_every 600000\\\n",
    "    --eval_every 3500 \\\n",
    "    --warmup_steps 9000 \\\n",
    "    --seed 42\\\n",
    "    --tags relora_bert_medium \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 15625 \\\n",
    "    --restart_warmup_steps 500 \\\n",
    "    --scheduler cosine_restarts \\\n",
    "    --reset_optimizer_on_relora False \\\n",
    "    --relora 15625\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.9\\\n",
    "    --lora_alpha 64\\\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a7b65-cbe7-45ff-b567-36cd2dce2c6c",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376405e-8f50-4bb0-ac4c-1a7410f447b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "-min_lr_ratio 0.000000000000000000000000000000000000000000001\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a416de-adc8-49ca-bb9d-7e9f192a7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-22 00:49:29.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:29.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240922_004930-ptz0xf8p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroyal-snow-735\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/ptz0xf8p\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_name_or_path             bert_c4\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdataset_path                   /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mbatch_size                     64\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtotal_batch_size               64\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_peft                       False\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_r                         None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_alpha                     64.0\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrelora                         None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mreset_optimizer_on_relora      False\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlr                             0.0005\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mscheduler                      linear\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcycle_length                   15625\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrestart_warmup_steps           500\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmin_lr_ratio                   1e-45\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mweight_decay                   0.03\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmup_steps                   9000\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1meval_every                     3000\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mnum_training_steps             62500\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_every                     600000\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_dir                       checkpoint/royal-snow-735\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtags                           ['relora_bert_medium']\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrun_name                       royal-snow-735\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m473\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:31.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:32.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:32.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1mUsing HuggingFace model bert_c4 revision None\u001b[0m\n",
      "trainable params: 41404218 || all params: 41404218 || trainable%: 100.00\n",
      "\u001b[32m2024-09-22 00:49:32.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m630\u001b[0m - \u001b[1m\n",
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 512)\n",
      "      (token_type_embeddings): Embedding(1, 512)\n",
      "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-7): 8 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:32.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mTotal params  before LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:32.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m632\u001b[0m - \u001b[1mTotal params  after  LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:32.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mTrainable params: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:33.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mIn total, added 0.00M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:33.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mSaving model to checkpoint/royal-snow-735 every 600000 update steps\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:33.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m657\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-09-22 00:49:33.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:33.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[1mScheduler will run for 62500 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 00:49:33.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1mFull training set size: 4000000\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:33.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m763\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:33.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mTrain set size after shard: 4000000\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 00:49:33.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m772\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-09-22 00:49:33.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m871\u001b[0m - \u001b[1mStarting training at update step 0 with 62500 update steps\u001b[0m\n",
      "Update steps:   0%|                                   | 0/62500 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 00:49:49.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m904\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Update steps:   5%|‚ñà                     | 3000/62500 [05:16<1:45:07,  9.43it/s]\u001b[32m2024-09-22 00:54:50.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 3000\u001b[0m\n",
      "\u001b[32m2024-09-22 00:55:00.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8095232.0 tokens, eval loss: 6.7313\u001b[0m\n",
      "\u001b[32m2024-09-22 00:55:00.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 838.25\u001b[0m\n",
      "\u001b[32m2024-09-22 00:55:00.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.03 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 00:55:00.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 3000: 6.731311321258545, Perplexity: 838.2457550391559\u001b[0m\n",
      "Update steps:  10%|‚ñà‚ñà                    | 6000/62500 [10:26<1:34:01, 10.02it/s]\u001b[32m2024-09-22 00:59:59.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 6000\u001b[0m\n",
      "\u001b[32m2024-09-22 01:00:11.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9126720.0 tokens, eval loss: 6.4367\u001b[0m\n",
      "\u001b[32m2024-09-22 01:00:11.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 624.33\u001b[0m\n",
      "\u001b[32m2024-09-22 01:00:11.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.28 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:00:11.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 6000: 6.436684608459473, Perplexity: 624.3334548807399\u001b[0m\n",
      "Update steps:  14%|‚ñà‚ñà‚ñà‚ñè                  | 9000/62500 [15:37<1:25:34, 10.42it/s]\u001b[32m2024-09-22 01:05:10.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 9000\u001b[0m\n",
      "\u001b[32m2024-09-22 01:05:20.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8093952.0 tokens, eval loss: 4.8345\u001b[0m\n",
      "\u001b[32m2024-09-22 01:05:20.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 125.77\u001b[0m\n",
      "\u001b[32m2024-09-22 01:05:20.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.03 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:05:20.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 9000: 4.834456920623779, Perplexity: 125.77026142135591\u001b[0m\n",
      "Update steps:  19%|‚ñà‚ñà‚ñà‚ñà                 | 12000/62500 [20:46<1:21:02, 10.39it/s]\u001b[32m2024-09-22 01:10:19.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 12000\u001b[0m\n",
      "Update steps:  19%|‚ñà‚ñà‚ñà‚ñà                 | 12000/62500 [20:59<1:21:02, 10.39it/s]\u001b[32m2024-09-22 01:10:34.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11518720.0 tokens, eval loss: 4.1125\u001b[0m\n",
      "\u001b[32m2024-09-22 01:10:34.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 61.10\u001b[0m\n",
      "\u001b[32m2024-09-22 01:10:34.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.25 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:10:34.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 12000: 4.112521648406982, Perplexity: 61.100597697141744\u001b[0m\n",
      "Update steps:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà                | 15000/62500 [26:00<1:23:31,  9.48it/s]\u001b[32m2024-09-22 01:15:33.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 15000\u001b[0m\n",
      "\u001b[32m2024-09-22 01:15:46.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10548160.0 tokens, eval loss: 3.8191\u001b[0m\n",
      "\u001b[32m2024-09-22 01:15:46.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 45.56\u001b[0m\n",
      "\u001b[32m2024-09-22 01:15:46.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.07 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:15:46.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 15000: 3.8191120624542236, Perplexity: 45.56373260464867\u001b[0m\n",
      "Update steps:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 17999/62500 [31:12<1:17:36,  9.56it/s]\u001b[32m2024-09-22 01:20:45.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 18000\u001b[0m\n",
      "\u001b[32m2024-09-22 01:20:59.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11498880.0 tokens, eval loss: 3.6529\u001b[0m\n",
      "\u001b[32m2024-09-22 01:20:59.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 38.59\u001b[0m\n",
      "\u001b[32m2024-09-22 01:20:59.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.28 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:20:59.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 18000: 3.6529223918914795, Perplexity: 38.587268555307894\u001b[0m\n",
      "Update steps:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 21000/62500 [36:26<1:06:44, 10.36it/s]\u001b[32m2024-09-22 01:25:59.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 21000\u001b[0m\n",
      "Update steps:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 21000/62500 [36:39<1:06:44, 10.36it/s]\u001b[32m2024-09-22 01:26:13.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11282240.0 tokens, eval loss: 3.5272\u001b[0m\n",
      "\u001b[32m2024-09-22 01:26:13.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 34.03\u001b[0m\n",
      "\u001b[32m2024-09-22 01:26:13.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.95 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:26:13.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 21000: 3.527242660522461, Perplexity: 34.030005853488774\u001b[0m\n",
      "Update steps:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 24000/62500 [41:40<1:01:49, 10.38it/s]\u001b[32m2024-09-22 01:31:13.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 24000\u001b[0m\n",
      "Update steps:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 24000/62500 [41:52<1:01:49, 10.38it/s]\u001b[32m2024-09-22 01:31:26.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10357376.0 tokens, eval loss: 3.4479\u001b[0m\n",
      "\u001b[32m2024-09-22 01:31:26.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 31.43\u001b[0m\n",
      "\u001b[32m2024-09-22 01:31:26.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.94 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:31:26.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 24000: 3.447863817214966, Perplexity: 31.43317353433202\u001b[0m\n",
      "Update steps:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 27000/62500 [46:53<59:08, 10.01it/s]\u001b[32m2024-09-22 01:36:26.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 27000\u001b[0m\n",
      "\u001b[32m2024-09-22 01:36:39.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10825920.0 tokens, eval loss: 3.3751\u001b[0m\n",
      "\u001b[32m2024-09-22 01:36:39.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 29.23\u001b[0m\n",
      "\u001b[32m2024-09-22 01:36:39.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.41 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:36:39.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 27000: 3.3751139640808105, Perplexity: 29.227614489660372\u001b[0m\n",
      "Update steps:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 29999/62500 [52:04<52:16, 10.36it/s]\u001b[32m2024-09-22 01:41:37.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 30000\u001b[0m\n",
      "\u001b[32m2024-09-22 01:41:50.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10265280.0 tokens, eval loss: 3.3329\u001b[0m\n",
      "\u001b[32m2024-09-22 01:41:50.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 28.02\u001b[0m\n",
      "\u001b[32m2024-09-22 01:41:50.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.64 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:41:50.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 30000: 3.3328769207000732, Perplexity: 28.018833826018138\u001b[0m\n",
      "Update steps:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 33000/62500 [57:15<47:22, 10.38it/s]\u001b[32m2024-09-22 01:46:48.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 33000\u001b[0m\n",
      "\u001b[32m2024-09-22 01:47:00.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9903424.0 tokens, eval loss: 3.2939\u001b[0m\n",
      "\u001b[32m2024-09-22 01:47:00.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 26.95\u001b[0m\n",
      "\u001b[32m2024-09-22 01:47:00.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.21 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:47:00.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 33000: 3.293916702270508, Perplexity: 26.948205322231793\u001b[0m\n",
      "Update steps:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 35999/62500 [1:02:24<42:16, 10.45it/s]\u001b[32m2024-09-22 01:51:58.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 36000\u001b[0m\n",
      "\u001b[32m2024-09-22 01:52:10.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9976768.0 tokens, eval loss: 3.2476\u001b[0m\n",
      "\u001b[32m2024-09-22 01:52:10.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.73\u001b[0m\n",
      "\u001b[32m2024-09-22 01:52:10.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.27 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:52:10.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 36000: 3.2476186752319336, Perplexity: 25.7289978087003\u001b[0m\n",
      "Update steps:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 39000/62500 [1:07:34<39:46,  9.85it/s]\u001b[32m2024-09-22 01:57:07.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 39000\u001b[0m\n",
      "\u001b[32m2024-09-22 01:57:19.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9761856.0 tokens, eval loss: 3.2381\u001b[0m\n",
      "\u001b[32m2024-09-22 01:57:19.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.49\u001b[0m\n",
      "\u001b[32m2024-09-22 01:57:19.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.03 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 01:57:19.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 39000: 3.238091230392456, Perplexity: 25.485030240530435\u001b[0m\n",
      "Update steps:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 42000/62500 [1:12:42<35:13,  9.70it/s]\u001b[32m2024-09-22 02:02:15.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 42000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:02:28.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10138048.0 tokens, eval loss: 3.2239\u001b[0m\n",
      "\u001b[32m2024-09-22 02:02:28.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.13\u001b[0m\n",
      "\u001b[32m2024-09-22 02:02:28.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.48 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:02:28.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 42000: 3.223917245864868, Perplexity: 25.126353758545992\u001b[0m\n",
      "Update steps:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 44999/62500 [1:17:52<28:03, 10.40it/s]\u001b[32m2024-09-22 02:07:25.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 45000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:07:38.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10683648.0 tokens, eval loss: 3.2011\u001b[0m\n",
      "\u001b[32m2024-09-22 02:07:38.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.56\u001b[0m\n",
      "\u001b[32m2024-09-22 02:07:38.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.16 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:07:38.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 45000: 3.2011139392852783, Perplexity: 24.559873172643094\u001b[0m\n",
      "Update steps:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 47999/62500 [1:23:03<24:47,  9.75it/s]\u001b[32m2024-09-22 02:12:36.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 48000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:12:49.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10103040.0 tokens, eval loss: 3.1841\u001b[0m\n",
      "\u001b[32m2024-09-22 02:12:49.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.15\u001b[0m\n",
      "\u001b[32m2024-09-22 02:12:49.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.46 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:12:49.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 48000: 3.184105157852173, Perplexity: 24.145672170582465\u001b[0m\n",
      "Update steps:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 50999/62500 [1:28:12<18:38, 10.28it/s]\u001b[32m2024-09-22 02:17:46.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 51000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:17:57.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9298112.0 tokens, eval loss: 3.1812\u001b[0m\n",
      "\u001b[32m2024-09-22 02:17:57.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.07\u001b[0m\n",
      "\u001b[32m2024-09-22 02:17:57.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.42 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:17:57.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 51000: 3.1811606884002686, Perplexity: 24.07468054410909\u001b[0m\n",
      "Update steps:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 53999/62500 [1:33:21<13:41, 10.35it/s]\u001b[32m2024-09-22 02:22:54.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 54000\u001b[0m\n",
      "Update steps:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 54000/62500 [1:33:32<13:41, 10.35it/s]\u001b[32m2024-09-22 02:23:06.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9359808.0 tokens, eval loss: 3.1784\u001b[0m\n",
      "\u001b[32m2024-09-22 02:23:06.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.01\u001b[0m\n",
      "\u001b[32m2024-09-22 02:23:06.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.53 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:23:06.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 54000: 3.1783816814422607, Perplexity: 24.007869716240613\u001b[0m\n",
      "Update steps:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 57000/62500 [1:38:30<08:58, 10.22it/s]\u001b[32m2024-09-22 02:28:03.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 57000\u001b[0m\n",
      "Update steps:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 57000/62500 [1:38:42<08:58, 10.22it/s]\u001b[32m2024-09-22 02:28:15.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9804288.0 tokens, eval loss: 3.1761\u001b[0m\n",
      "\u001b[32m2024-09-22 02:28:15.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 23.95\u001b[0m\n",
      "\u001b[32m2024-09-22 02:28:15.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.08 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:28:15.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 57000: 3.1761093139648438, Perplexity: 23.953376951137432\u001b[0m\n",
      "Update steps:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 59999/62500 [1:43:38<04:34,  9.11it/s]\u001b[32m2024-09-22 02:33:11.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 60000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:33:23.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9689600.0 tokens, eval loss: 3.1694\u001b[0m\n",
      "\u001b[32m2024-09-22 02:33:23.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 23.79\u001b[0m\n",
      "\u001b[32m2024-09-22 02:33:23.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.93 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:33:23.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 60000: 3.1693990230560303, Perplexity: 23.793180905689095\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:47:59<00:00,  9.56it/s]Warning: reached the end of the dataset. Training stopped, global_rank=0, update_step=62500\n",
      "\u001b[32m2024-09-22 02:37:32.343\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1081\u001b[0m - \u001b[33m\u001b[1mReached the end of the dataset. Training stopped\u001b[0m\n",
      "\u001b[32m2024-09-22 02:37:32.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1087\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:47:59<00:00,  9.65it/s]\n",
      "\u001b[32m2024-09-22 02:37:32.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1092\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/royal-snow-735/model_62500, update step 62500\u001b[0m\n",
      "\u001b[32m2024-09-22 02:37:32.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m243\u001b[0m - \u001b[1mSaving took 0.20 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:37:32.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1112\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:23.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 41441120.0 tokens, eval loss: 3.1770\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:23.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 23.98\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:23.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 51.10 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:23.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1131\u001b[0m - \u001b[1mFinal eval loss: 3.1770334243774414\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B sync reduced upload amount by 0.8%\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm ‚ñÜ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÅ‚ñÖ‚ñá‚ñà‚ñá‚ñÖ‚ñÑ‚ñá‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÅ‚ñÖ‚ñá‚ñà‚ñá‚ñÖ‚ñÑ‚ñá‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñá‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÅ‚ñà‚ñÜ‚ñá‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 3.17703\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 41441120.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm 1.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 3.32812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity 23.97552\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 8.745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 559.68012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 265288.37471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 1652536704\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 62500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mroyal-snow-735\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/ptz0xf8p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240922_004930-ptz0xf8p/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "\u001b[32m2024-09-22 02:38:33.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1154\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
      "Rank 0 finished successfully\n"
     ]
    }
   ],
   "source": [
    "#chocolate-galaxy-701\n",
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path bert_c4 \\\n",
    "    --dataset_path /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM \\\n",
    "    --batch_size 64\\\n",
    "    --lr 5e-4 \\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.03 \\\n",
    "    --num_training_steps 62500 \\\n",
    "    --save_every 600000\\\n",
    "    --eval_every 3000 \\\n",
    "    --warmup_steps 9000 \\\n",
    "    --seed 42 \\\n",
    "    --tags relora_bert_medium \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft False \\\n",
    "    --cycle_length 15625 \\\n",
    "    --restart_warmup_steps 500 \\\n",
    "    --scheduler linear \\\n",
    "    --reset_optimizer_on_relora False \\\n",
    "    --relora 15625\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.9\\\n",
    "    --lora_alpha 64 \\\n",
    "    --min_lr_ratio 0.000000000000000000000000000000000000000000001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df1af8f-09c0-4d83-8d5b-855b2f5b66e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-22 02:38:40.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:40.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240922_023841-yzbe6as6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlunar-terrain-736\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/yzbe6as6\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_name_or_path             bert_c4\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdataset_path                   /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mbatch_size                     64\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtotal_batch_size               64\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_r                         64\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_alpha                     128.0\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrelora                         62500\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mreset_optimizer_on_relora      False\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlr                             0.0005\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mscheduler                      linear\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcycle_length                   62500\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrestart_warmup_steps           500\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmin_lr_ratio                   1e-45\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mweight_decay                   0.02\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmup_steps                   9000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1meval_every                     3000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mnum_training_steps             62500\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_every                     600000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_dir                       checkpoint/lunar-terrain-736\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtags                           ['relora_bert_medium']\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrun_name                       lunar-terrain-736\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m473\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:42.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1mUsing HuggingFace model bert_c4 revision None\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m565\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 36685626 || all params: 42977082 || trainable%: 85.36\n",
      "\u001b[32m2024-09-22 02:38:43.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m630\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): BertForMaskedLM(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 512)\n",
      "        (token_type_embeddings): Embedding(1, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-7): 8 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (key): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (value): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mTotal params  before LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m632\u001b[0m - \u001b[1mTotal params  after  LoRA: 42.98M\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mTrainable params: 36.69M\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mIn total, added 1.57M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mSaving model to checkpoint/lunar-terrain-736 every 600000 update steps\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m657\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-09-22 02:38:43.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[1mScheduler will run for 62500 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 02:38:43.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1mFull training set size: 4000000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m763\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mTrain set size after shard: 4000000\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 02:38:43.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m772\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-09-22 02:38:43.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m871\u001b[0m - \u001b[1mStarting training at update step 0 with 62500 update steps\u001b[0m\n",
      "Update steps:   0%|                                   | 0/62500 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 02:38:59.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m904\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Update steps:   5%|‚ñà                     | 3000/62500 [05:42<1:48:30,  9.14it/s]\u001b[32m2024-09-22 02:44:25.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 3000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:44:39.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10294272.0 tokens, eval loss: 6.7391\u001b[0m\n",
      "\u001b[32m2024-09-22 02:44:39.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 844.76\u001b[0m\n",
      "\u001b[32m2024-09-22 02:44:39.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.98 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:44:39.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 3000: 6.739050388336182, Perplexity: 844.7581626405573\u001b[0m\n",
      "Update steps:  10%|‚ñà‚ñà                    | 5999/62500 [11:22<1:44:44,  8.99it/s]\u001b[32m2024-09-22 02:50:06.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 6000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:50:20.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10057856.0 tokens, eval loss: 6.4731\u001b[0m\n",
      "\u001b[32m2024-09-22 02:50:20.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 647.51\u001b[0m\n",
      "\u001b[32m2024-09-22 02:50:20.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.67 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:50:20.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 6000: 6.4731316566467285, Perplexity: 647.5083288933581\u001b[0m\n",
      "Update steps:  14%|‚ñà‚ñà‚ñà‚ñè                  | 9000/62500 [17:03<1:40:29,  8.87it/s]\u001b[32m2024-09-22 02:55:46.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 9000\u001b[0m\n",
      "\u001b[32m2024-09-22 02:56:02.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11892160.0 tokens, eval loss: 6.2363\u001b[0m\n",
      "\u001b[32m2024-09-22 02:56:02.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 510.97\u001b[0m\n",
      "\u001b[32m2024-09-22 02:56:02.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 16.07 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 02:56:02.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 9000: 6.23631477355957, Perplexity: 510.9719894625763\u001b[0m\n",
      "Update steps:  19%|‚ñà‚ñà‚ñà‚ñà                 | 11999/62500 [22:46<1:26:45,  9.70it/s]\u001b[32m2024-09-22 03:01:29.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 12000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:01:40.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8459968.0 tokens, eval loss: 5.0675\u001b[0m\n",
      "\u001b[32m2024-09-22 03:01:40.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 158.77\u001b[0m\n",
      "\u001b[32m2024-09-22 03:01:40.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.47 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:01:40.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 12000: 5.067480564117432, Perplexity: 158.7738025914833\u001b[0m\n",
      "Update steps:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà                | 15000/62500 [28:24<1:25:28,  9.26it/s]\u001b[32m2024-09-22 03:07:07.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 15000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:07:18.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8081344.0 tokens, eval loss: 4.3314\u001b[0m\n",
      "\u001b[32m2024-09-22 03:07:18.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 76.05\u001b[0m\n",
      "\u001b[32m2024-09-22 03:07:18.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.98 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:07:18.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 15000: 4.331392765045166, Perplexity: 76.05013280926264\u001b[0m\n",
      "Update steps:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 18000/62500 [34:01<1:15:08,  9.87it/s]\u001b[32m2024-09-22 03:12:44.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 18000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:12:57.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9698816.0 tokens, eval loss: 4.0148\u001b[0m\n",
      "\u001b[32m2024-09-22 03:12:57.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 55.41\u001b[0m\n",
      "\u001b[32m2024-09-22 03:12:57.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.19 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:12:57.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 18000: 4.014768600463867, Perplexity: 55.41047196137316\u001b[0m\n",
      "Update steps:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 21000/62500 [39:41<1:13:39,  9.39it/s]\u001b[32m2024-09-22 03:18:24.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 21000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:18:38.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10771584.0 tokens, eval loss: 3.8374\u001b[0m\n",
      "\u001b[32m2024-09-22 03:18:38.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 46.40\u001b[0m\n",
      "\u001b[32m2024-09-22 03:18:38.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.56 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:18:38.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 21000: 3.837399959564209, Perplexity: 46.40466344973997\u001b[0m\n",
      "Update steps:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 23999/62500 [45:23<1:05:39,  9.77it/s]\u001b[32m2024-09-22 03:24:07.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 24000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:24:20.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9764544.0 tokens, eval loss: 3.7132\u001b[0m\n",
      "\u001b[32m2024-09-22 03:24:20.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 40.98\u001b[0m\n",
      "\u001b[32m2024-09-22 03:24:20.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.24 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:24:20.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 24000: 3.7131636142730713, Perplexity: 40.98325686993817\u001b[0m\n",
      "Update steps:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 27000/62500 [51:05<1:02:16,  9.50it/s]\u001b[32m2024-09-22 03:29:48.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 27000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:30:03.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10676672.0 tokens, eval loss: 3.6400\u001b[0m\n",
      "\u001b[32m2024-09-22 03:30:03.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 38.09\u001b[0m\n",
      "\u001b[32m2024-09-22 03:30:03.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.50 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:30:03.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 27000: 3.639961004257202, Perplexity: 38.09035133489351\u001b[0m\n",
      "Update steps:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 30000/62500 [56:45<58:41,  9.23it/s]\u001b[32m2024-09-22 03:35:29.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 30000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:35:40.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8097664.0 tokens, eval loss: 3.5744\u001b[0m\n",
      "\u001b[32m2024-09-22 03:35:40.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 35.67\u001b[0m\n",
      "\u001b[32m2024-09-22 03:35:40.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.99 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:35:40.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 30000: 3.574448585510254, Perplexity: 35.674943710151254\u001b[0m\n",
      "Update steps:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 32999/62500 [1:02:24<55:44,  8.82it/s]\u001b[32m2024-09-22 03:41:07.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 33000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:41:23.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11561856.0 tokens, eval loss: 3.5351\u001b[0m\n",
      "\u001b[32m2024-09-22 03:41:23.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 34.30\u001b[0m\n",
      "\u001b[32m2024-09-22 03:41:23.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.64 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:41:23.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 33000: 3.5351204872131348, Perplexity: 34.299147073919436\u001b[0m\n",
      "Update steps:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 36000/62500 [1:08:07<47:17,  9.34it/s]\u001b[32m2024-09-22 03:46:50.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 36000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:47:04.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9914560.0 tokens, eval loss: 3.5141\u001b[0m\n",
      "\u001b[32m2024-09-22 03:47:04.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 33.59\u001b[0m\n",
      "\u001b[32m2024-09-22 03:47:04.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.38 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:47:04.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 36000: 3.5141170024871826, Perplexity: 33.586258244229334\u001b[0m\n",
      "Update steps:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 39000/62500 [1:13:47<42:26,  9.23it/s]\u001b[32m2024-09-22 03:52:31.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 39000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:52:47.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11786304.0 tokens, eval loss: 3.4783\u001b[0m\n",
      "\u001b[32m2024-09-22 03:52:47.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 32.40\u001b[0m\n",
      "\u001b[32m2024-09-22 03:52:47.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 16.03 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:52:47.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 39000: 3.478271484375, Perplexity: 32.40366340217672\u001b[0m\n",
      "Update steps:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 42000/62500 [1:19:30<36:33,  9.35it/s]\u001b[32m2024-09-22 03:58:13.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 42000\u001b[0m\n",
      "\u001b[32m2024-09-22 03:58:27.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10233600.0 tokens, eval loss: 3.4552\u001b[0m\n",
      "\u001b[32m2024-09-22 03:58:27.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 31.67\u001b[0m\n",
      "\u001b[32m2024-09-22 03:58:27.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.87 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 03:58:27.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 42000: 3.4552464485168457, Perplexity: 31.666091782058157\u001b[0m\n",
      "Update steps:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 45000/62500 [1:25:12<32:04,  9.09it/s]\u001b[32m2024-09-22 04:03:55.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 45000\u001b[0m\n",
      "\u001b[32m2024-09-22 04:04:10.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10827456.0 tokens, eval loss: 3.4448\u001b[0m\n",
      "\u001b[32m2024-09-22 04:04:10.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 31.34\u001b[0m\n",
      "\u001b[32m2024-09-22 04:04:10.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.69 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:04:10.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 45000: 3.4447991847991943, Perplexity: 31.336989870854126\u001b[0m\n",
      "Update steps:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 48000/62500 [1:30:54<26:14,  9.21it/s]\u001b[32m2024-09-22 04:09:37.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 48000\u001b[0m\n",
      "\u001b[32m2024-09-22 04:09:49.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8874624.0 tokens, eval loss: 3.4375\u001b[0m\n",
      "\u001b[32m2024-09-22 04:09:49.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 31.11\u001b[0m\n",
      "\u001b[32m2024-09-22 04:09:49.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.02 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:09:49.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 48000: 3.437546730041504, Perplexity: 31.110541913915043\u001b[0m\n",
      "Update steps:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 51000/62500 [1:36:33<18:54, 10.14it/s]\u001b[32m2024-09-22 04:15:17.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 51000\u001b[0m\n",
      "\u001b[32m2024-09-22 04:15:28.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8136320.0 tokens, eval loss: 3.4244\u001b[0m\n",
      "\u001b[32m2024-09-22 04:15:28.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 30.71\u001b[0m\n",
      "\u001b[32m2024-09-22 04:15:28.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.02 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:15:28.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 51000: 3.4244279861450195, Perplexity: 30.705076098250387\u001b[0m\n",
      "Update steps:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 54000/62500 [1:42:12<15:10,  9.33it/s]\u001b[32m2024-09-22 04:20:56.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 54000\u001b[0m\n",
      "\u001b[32m2024-09-22 04:21:11.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11175808.0 tokens, eval loss: 3.4263\u001b[0m\n",
      "\u001b[32m2024-09-22 04:21:11.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 30.76\u001b[0m\n",
      "\u001b[32m2024-09-22 04:21:11.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.17 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:21:11.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 54000: 3.4262914657592773, Perplexity: 30.76234772729707\u001b[0m\n",
      "Update steps:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 57000/62500 [1:47:55<09:51,  9.30it/s]\u001b[32m2024-09-22 04:26:39.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 57000\u001b[0m\n",
      "\u001b[32m2024-09-22 04:26:52.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9911104.0 tokens, eval loss: 3.4117\u001b[0m\n",
      "\u001b[32m2024-09-22 04:26:52.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 30.32\u001b[0m\n",
      "\u001b[32m2024-09-22 04:26:52.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.44 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:26:52.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 57000: 3.4117376804351807, Perplexity: 30.31788130215034\u001b[0m\n",
      "Update steps:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 60000/62500 [1:53:36<04:19,  9.65it/s]\u001b[32m2024-09-22 04:32:19.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 60000\u001b[0m\n",
      "\u001b[32m2024-09-22 04:32:34.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10596096.0 tokens, eval loss: 3.4144\u001b[0m\n",
      "\u001b[32m2024-09-22 04:32:34.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 30.40\u001b[0m\n",
      "\u001b[32m2024-09-22 04:32:34.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.35 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:32:34.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 60000: 3.4144105911254883, Perplexity: 30.399026690066147\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:23<00:00,  9.26it/s]Warning: reached the end of the dataset. Training stopped, global_rank=0, update_step=62500\n",
      "\u001b[32m2024-09-22 04:37:07.263\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1081\u001b[0m - \u001b[33m\u001b[1mReached the end of the dataset. Training stopped\u001b[0m\n",
      "\u001b[32m2024-09-22 04:37:07.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1087\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:24<00:00,  8.80it/s]\n",
      "\u001b[32m2024-09-22 04:37:07.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1092\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/lunar-terrain-736/model_62500, update step 62500\u001b[0m\n",
      "\u001b[32m2024-09-22 04:37:07.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m243\u001b[0m - \u001b[1mSaving took 0.19 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:37:07.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1112\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:03.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 41385760.0 tokens, eval loss: 3.4145\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:03.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 30.40\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:03.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 56.15 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:03.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1131\u001b[0m - \u001b[1mFinal eval loss: 3.4145073890686035\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss ‚ñà‚ñá‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm ‚ñá‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity ‚ñà‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ‚ñÑ‚ñá‚ñà‚ñÇ‚ñà‚ñÉ‚ñà‚ñÇ‚ñá‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñà‚ñÇ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 3.41451\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 41385760.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm 1.91406\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 3.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity 30.40197\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 9.22462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 590.37546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 242053.93823\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 1652867648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 62500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlunar-terrain-736\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/yzbe6as6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240922_023841-yzbe6as6/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "\u001b[32m2024-09-22 04:38:10.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1154\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
      "Rank 0 finished successfully\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path bert_c4 \\\n",
    "    --dataset_path /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM \\\n",
    "    --batch_size 64\\\n",
    "    --lr 5e-4 \\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.02 \\\n",
    "    --num_training_steps 62500 \\\n",
    "    --save_every 600000\\\n",
    "    --eval_every 3000 \\\n",
    "    --warmup_steps 9000 \\\n",
    "    --seed 42 \\\n",
    "    --tags relora_bert_medium \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 62500 \\\n",
    "    --restart_warmup_steps 500 \\\n",
    "    --scheduler linear \\\n",
    "    --reset_optimizer_on_relora False \\\n",
    "    --relora 62500\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.9\\\n",
    "    --lora_alpha 128 \\\n",
    "    --min_lr_ratio 0.000000000000000000000000000000000000000000001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "673e716a-76d9-4610-a650-d86f5674308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-22 04:38:21.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:21.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240922_043822-tnd7kgy2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdesert-durian-737\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/tnd7kgy2\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_name_or_path             bert_c4\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdataset_path                   /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mbatch_size                     64\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtotal_batch_size               64\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_r                         64\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_alpha                     64.0\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrelora                         15625\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mreset_optimizer_on_relora      False\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlr                             0.001\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mscheduler                      cosine_restarts\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcycle_length                   15625\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrestart_warmup_steps           500\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmin_lr_ratio                   0.1\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mweight_decay                   0.02\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmup_steps                   9000\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1meval_every                     3500\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mnum_training_steps             62500\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_every                     600000\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_dir                       checkpoint/desert-durian-737\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtags                           ['relora_bert_medium']\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrun_name                       desert-durian-737\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m473\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:23.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1mUsing HuggingFace model bert_c4 revision None\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m565\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 36685626 || all params: 42977082 || trainable%: 85.36\n",
      "\u001b[32m2024-09-22 04:38:24.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m630\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): BertForMaskedLM(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 512)\n",
      "        (token_type_embeddings): Embedding(1, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-7): 8 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (key): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (value): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mTotal params  before LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m632\u001b[0m - \u001b[1mTotal params  after  LoRA: 42.98M\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mTrainable params: 36.69M\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mIn total, added 1.57M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mSaving model to checkpoint/desert-durian-737 every 600000 update steps\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m657\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-09-22 04:38:24.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[1mScheduler will run for 62500 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 04:38:24.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1mFull training set size: 4000000\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m763\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mTrain set size after shard: 4000000\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 04:38:24.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m772\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-09-22 04:38:24.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m871\u001b[0m - \u001b[1mStarting training at update step 0 with 62500 update steps\u001b[0m\n",
      "Update steps:   0%|                                   | 0/62500 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 04:38:40.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m904\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Update steps:   6%|‚ñà‚ñè                    | 3500/62500 [06:36<1:45:25,  9.33it/s]\u001b[32m2024-09-22 04:45:00.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 3500\u001b[0m\n",
      "\u001b[32m2024-09-22 04:45:14.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10204864.0 tokens, eval loss: 6.5048\u001b[0m\n",
      "\u001b[32m2024-09-22 04:45:14.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 668.33\u001b[0m\n",
      "\u001b[32m2024-09-22 04:45:14.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.80 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:45:14.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 3500: 6.504788875579834, Perplexity: 668.334552713474\u001b[0m\n",
      "Update steps:  11%|‚ñà‚ñà‚ñç                   | 7000/62500 [13:11<1:34:44,  9.76it/s]\u001b[32m2024-09-22 04:51:35.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 7000\u001b[0m\n",
      "\u001b[32m2024-09-22 04:51:46.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8358464.0 tokens, eval loss: 6.1349\u001b[0m\n",
      "\u001b[32m2024-09-22 04:51:46.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 461.70\u001b[0m\n",
      "\u001b[32m2024-09-22 04:51:46.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.31 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:51:46.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 7000: 6.134915828704834, Perplexity: 461.7002304769535\u001b[0m\n",
      "Update steps:  17%|‚ñà‚ñà‚ñà‚ñå                 | 10500/62500 [19:44<1:37:50,  8.86it/s]\u001b[32m2024-09-22 04:58:08.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 10500\u001b[0m\n",
      "\u001b[32m2024-09-22 04:58:23.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10905216.0 tokens, eval loss: 4.3326\u001b[0m\n",
      "\u001b[32m2024-09-22 04:58:23.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 76.15\u001b[0m\n",
      "\u001b[32m2024-09-22 04:58:23.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.79 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 04:58:23.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 10500: 4.332649230957031, Perplexity: 76.14574726430308\u001b[0m\n",
      "Update steps:  22%|‚ñà‚ñà‚ñà‚ñà‚ñã                | 14000/62500 [26:19<1:26:22,  9.36it/s]\u001b[32m2024-09-22 05:04:43.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 14000\u001b[0m\n",
      "\u001b[32m2024-09-22 05:04:54.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8114240.0 tokens, eval loss: 3.8303\u001b[0m\n",
      "\u001b[32m2024-09-22 05:04:54.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 46.08\u001b[0m\n",
      "\u001b[32m2024-09-22 05:04:54.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.00 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 05:04:54.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 14000: 3.8303205966949463, Perplexity: 46.07730809919164\u001b[0m\n",
      "Update steps:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 15626/62500 [29:27<1:29:51,  8.69it/s]\u001b[32m2024-09-22 05:07:51.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1015\u001b[0m - \u001b[1margs.resume_from=None, local_step=15626, args.relora=15625, thresh: 15626\u001b[0m\n",
      "\u001b[32m2024-09-22 05:07:51.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1016\u001b[0m - \u001b[1mPerforming lora reset at update step 15626. Current lr is 1.9223699443912782e-06\u001b[0m\n",
      "\u001b[32m2024-09-22 05:07:51.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1027\u001b[0m - \u001b[1mLoRA reset took 0.00s\u001b[0m\n",
      "\u001b[32m2024-09-22 05:07:51.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1036\u001b[0m - \u001b[1mPerforming optimizer reset at update step 15626. Current lr is 1.9223699443912782e-06\u001b[0m\n",
      "\u001b[32m2024-09-22 05:07:51.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-09-22 05:07:51.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.08\u001b[0m\n",
      "Update steps:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 15627/62500 [29:27<1:31:53,  8.50it/s]\u001b[32m2024-09-22 05:07:51.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1050\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 3.8447398887825564e-06\u001b[0m\n",
      "Update steps:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 17500/62500 [32:50<1:19:59,  9.38it/s]\u001b[32m2024-09-22 05:11:15.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 17500\u001b[0m\n",
      "\u001b[32m2024-09-22 05:11:28.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10009984.0 tokens, eval loss: 3.5411\u001b[0m\n",
      "\u001b[32m2024-09-22 05:11:28.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 34.51\u001b[0m\n",
      "\u001b[32m2024-09-22 05:11:28.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.55 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 05:11:28.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 17500: 3.541144371032715, Perplexity: 34.506384712059\u001b[0m\n",
      "Update steps:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 21000/62500 [39:25<1:12:52,  9.49it/s]\u001b[32m2024-09-22 05:17:49.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 21000\u001b[0m\n",
      "\u001b[32m2024-09-22 05:18:00.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8103104.0 tokens, eval loss: 3.3660\u001b[0m\n",
      "\u001b[32m2024-09-22 05:18:00.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 28.96\u001b[0m\n",
      "\u001b[32m2024-09-22 05:18:00.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.93 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 05:18:00.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 21000: 3.3660130500793457, Perplexity: 28.962823232596335\u001b[0m\n",
      "Update steps:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 24500/62500 [45:58<1:07:29,  9.38it/s]\u001b[32m2024-09-22 05:24:23.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 24500\u001b[0m\n",
      "\u001b[32m2024-09-22 05:24:37.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11018304.0 tokens, eval loss: 3.2627\u001b[0m\n",
      "\u001b[32m2024-09-22 05:24:37.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 26.12\u001b[0m\n",
      "\u001b[32m2024-09-22 05:24:37.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.89 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 05:24:37.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 24500: 3.262702226638794, Perplexity: 26.12002408867343\u001b[0m\n",
      "Update steps:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 28000/62500 [52:36<1:02:28,  9.20it/s]\u001b[32m2024-09-22 05:31:00.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 28000\u001b[0m\n",
      "\u001b[32m2024-09-22 05:31:15.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11167232.0 tokens, eval loss: 3.1784\u001b[0m\n",
      "\u001b[32m2024-09-22 05:31:15.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.01\u001b[0m\n",
      "\u001b[32m2024-09-22 05:31:15.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.16 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 05:31:15.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 28000: 3.1783761978149414, Perplexity: 24.00773806639132\u001b[0m\n",
      "Update steps:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 31250/62500 [58:45<54:51,  9.49it/s]\u001b[32m2024-09-22 05:37:09.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1015\u001b[0m - \u001b[1margs.resume_from=None, local_step=31251, args.relora=15625, thresh: 31251\u001b[0m\n",
      "\u001b[32m2024-09-22 05:37:09.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1016\u001b[0m - \u001b[1mPerforming lora reset at update step 31251. Current lr is 1.3094584085039247e-06\u001b[0m\n",
      "\u001b[32m2024-09-22 05:37:09.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1027\u001b[0m - \u001b[1mLoRA reset took 0.00s\u001b[0m\n",
      "\u001b[32m2024-09-22 05:37:09.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1036\u001b[0m - \u001b[1mPerforming optimizer reset at update step 31251. Current lr is 1.3094584085039247e-06\u001b[0m\n",
      "\u001b[32m2024-09-22 05:37:09.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-09-22 05:37:09.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.09\u001b[0m\n",
      "Update steps:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 31252/62500 [58:45<53:46,  9.68it/s]\u001b[32m2024-09-22 05:37:09.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1050\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 2.6189168170078494e-06\u001b[0m\n",
      "Update steps:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 31500/62500 [59:12<53:02,  9.74it/s]\u001b[32m2024-09-22 05:37:36.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 31500\u001b[0m\n",
      "\u001b[32m2024-09-22 05:37:51.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10895040.0 tokens, eval loss: 3.0950\u001b[0m\n",
      "\u001b[32m2024-09-22 05:37:51.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 22.09\u001b[0m\n",
      "\u001b[32m2024-09-22 05:37:51.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.75 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 05:37:51.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 31500: 3.0950393676757812, Perplexity: 22.088108077907048\u001b[0m\n",
      "Update steps:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 35000/62500 [1:05:49<52:17,  8.77it/s]\u001b[32m2024-09-22 05:44:14.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 35000\u001b[0m\n",
      "\u001b[32m2024-09-22 05:44:28.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10333248.0 tokens, eval loss: 3.0387\u001b[0m\n",
      "\u001b[32m2024-09-22 05:44:28.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 20.88\u001b[0m\n",
      "\u001b[32m2024-09-22 05:44:28.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.99 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 05:44:28.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 35000: 3.038705587387085, Perplexity: 20.878200730428574\u001b[0m\n",
      "Update steps:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 38500/62500 [1:12:25<47:32,  8.41it/s]\u001b[32m2024-09-22 05:50:49.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 38500\u001b[0m\n",
      "\u001b[32m2024-09-22 05:51:04.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11246208.0 tokens, eval loss: 3.0034\u001b[0m\n",
      "\u001b[32m2024-09-22 05:51:04.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 20.15\u001b[0m\n",
      "\u001b[32m2024-09-22 05:51:04.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.20 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 05:51:04.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 38500: 3.0034143924713135, Perplexity: 20.15423404196339\u001b[0m\n",
      "Update steps:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 42000/62500 [1:19:01<36:35,  9.34it/s]\u001b[32m2024-09-22 05:57:26.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 42000\u001b[0m\n",
      "\u001b[32m2024-09-22 05:57:37.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8049536.0 tokens, eval loss: 2.9458\u001b[0m\n",
      "\u001b[32m2024-09-22 05:57:37.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 19.03\u001b[0m\n",
      "\u001b[32m2024-09-22 05:57:37.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.92 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 05:57:37.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 42000: 2.9458231925964355, Perplexity: 19.02631826601629\u001b[0m\n",
      "Update steps:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 45499/62500 [1:25:35<28:57,  9.78it/s]\u001b[32m2024-09-22 06:03:59.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 45500\u001b[0m\n",
      "\u001b[32m2024-09-22 06:04:14.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11015936.0 tokens, eval loss: 2.9463\u001b[0m\n",
      "\u001b[32m2024-09-22 06:04:14.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 19.04\u001b[0m\n",
      "\u001b[32m2024-09-22 06:04:14.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.93 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 06:04:14.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 45500: 2.946337938308716, Perplexity: 19.03611450283335\u001b[0m\n",
      "Update steps:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 46875/62500 [1:28:19<26:22,  9.87it/s]\u001b[32m2024-09-22 06:06:44.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1015\u001b[0m - \u001b[1margs.resume_from=None, local_step=46876, args.relora=15625, thresh: 46876\u001b[0m\n",
      "\u001b[32m2024-09-22 06:06:44.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1016\u001b[0m - \u001b[1mPerforming lora reset at update step 46876. Current lr is 5.322436819527499e-07\u001b[0m\n",
      "\u001b[32m2024-09-22 06:06:44.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1027\u001b[0m - \u001b[1mLoRA reset took 0.00s\u001b[0m\n",
      "\u001b[32m2024-09-22 06:06:44.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1036\u001b[0m - \u001b[1mPerforming optimizer reset at update step 46876. Current lr is 5.322436819527499e-07\u001b[0m\n",
      "\u001b[32m2024-09-22 06:06:44.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-09-22 06:06:44.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.05\u001b[0m\n",
      "Update steps:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 46877/62500 [1:28:19<26:23,  9.87it/s]\u001b[32m2024-09-22 06:06:44.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1050\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 1.0644873639054998e-06\u001b[0m\n",
      "Update steps:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 49000/62500 [1:32:12<27:24,  8.21it/s]\u001b[32m2024-09-22 06:10:36.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 49000\u001b[0m\n",
      "\u001b[32m2024-09-22 06:10:50.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10275584.0 tokens, eval loss: 2.9295\u001b[0m\n",
      "\u001b[32m2024-09-22 06:10:50.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 18.72\u001b[0m\n",
      "\u001b[32m2024-09-22 06:10:50.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.95 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 06:10:50.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 49000: 2.92954683303833, Perplexity: 18.719145675926885\u001b[0m\n",
      "Update steps:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 52500/62500 [1:38:48<17:38,  9.45it/s]\u001b[32m2024-09-22 06:17:13.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 52500\u001b[0m\n",
      "\u001b[32m2024-09-22 06:17:26.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10080960.0 tokens, eval loss: 2.9127\u001b[0m\n",
      "\u001b[32m2024-09-22 06:17:26.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 18.41\u001b[0m\n",
      "\u001b[32m2024-09-22 06:17:26.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.63 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 06:17:26.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 52500: 2.91269850730896, Perplexity: 18.406401418848812\u001b[0m\n",
      "Update steps:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 55999/62500 [1:45:24<11:28,  9.45it/s]\u001b[32m2024-09-22 06:23:49.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 56000\u001b[0m\n",
      "\u001b[32m2024-09-22 06:24:02.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9750208.0 tokens, eval loss: 2.9143\u001b[0m\n",
      "\u001b[32m2024-09-22 06:24:02.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 18.44\u001b[0m\n",
      "\u001b[32m2024-09-22 06:24:02.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.21 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 06:24:02.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 56000: 2.914275884628296, Perplexity: 18.435458169682647\u001b[0m\n",
      "Update steps:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 59500/62500 [1:52:00<05:49,  8.58it/s]\u001b[32m2024-09-22 06:30:24.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 59500\u001b[0m\n",
      "\u001b[32m2024-09-22 06:30:38.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10289344.0 tokens, eval loss: 2.9138\u001b[0m\n",
      "\u001b[32m2024-09-22 06:30:38.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 18.43\u001b[0m\n",
      "\u001b[32m2024-09-22 06:30:38.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.94 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 06:30:38.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 59500: 2.91382098197937, Perplexity: 18.42707373812175\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:57:41<00:00,  9.24it/s]Warning: reached the end of the dataset. Training stopped, global_rank=0, update_step=62500\n",
      "\u001b[32m2024-09-22 06:36:06.288\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1081\u001b[0m - \u001b[33m\u001b[1mReached the end of the dataset. Training stopped\u001b[0m\n",
      "\u001b[32m2024-09-22 06:36:06.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1087\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:57:42<00:00,  8.85it/s]\n",
      "\u001b[32m2024-09-22 06:36:06.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1092\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/desert-durian-737/model_62500, update step 62500\u001b[0m\n",
      "\u001b[32m2024-09-22 06:36:06.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m243\u001b[0m - \u001b[1mSaving took 0.19 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 06:36:06.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1112\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:02.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 41435328.0 tokens, eval loss: 2.9131\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:02.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 18.41\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:02.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 56.27 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:02.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1131\u001b[0m - \u001b[1mFinal eval loss: 2.9131078720092773\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm ‚ñÅ‚ñÇ‚ñÜ‚ñÑ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ‚ñÑ‚ñà‚ñà‚ñÇ‚ñà‚ñÉ‚ñà‚ñÇ‚ñá‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñà‚ñÇ‚ñÖ‚ñÉ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 2.91311\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 41435328.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm 1.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 3.07812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity 18.41394\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 9.21835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 589.97452\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 241889.55254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 1652867648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 62500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdesert-durian-737\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/tnd7kgy2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240922_043822-tnd7kgy2/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "\u001b[32m2024-09-22 06:37:10.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1154\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
      "Rank 0 finished successfully\n"
     ]
    }
   ],
   "source": [
    "#chocolate-galaxy-701\n",
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path bert_c4 \\\n",
    "    --dataset_path /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM \\\n",
    "    --batch_size 64\\\n",
    "    --lr 1e-3\\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.02\\\n",
    "    --num_training_steps 62500 \\\n",
    "    --save_every 600000\\\n",
    "    --eval_every 3500 \\\n",
    "    --warmup_steps 9000 \\\n",
    "    --seed 42\\\n",
    "    --tags relora_bert_medium \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 15625 \\\n",
    "    --restart_warmup_steps 500 \\\n",
    "    --scheduler cosine_restarts \\\n",
    "    --reset_optimizer_on_relora False \\\n",
    "    --relora 15625\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.9\\\n",
    "    --lora_alpha 64\\\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50c0c43-61a9-4cb1-b84d-2815145bb279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-22 06:37:17.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:17.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240922_063718-mibktbuq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msmart-surf-738\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/mibktbuq\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_name_or_path             bert_c4\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdataset_path                   /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mbatch_size                     64\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtotal_batch_size               64\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_r                         64\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_alpha                     128.0\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrelora                         62500\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mreset_optimizer_on_relora      False\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlr                             0.0006\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mscheduler                      linear\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcycle_length                   62500\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrestart_warmup_steps           500\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmin_lr_ratio                   1e-45\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mweight_decay                   0.02\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmup_steps                   9000\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1meval_every                     3000\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mnum_training_steps             62500\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_every                     600000\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_dir                       checkpoint/smart-surf-738\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtags                           ['relora_bert_medium']\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrun_name                       smart-surf-738\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m473\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:19.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1mUsing HuggingFace model bert_c4 revision None\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m565\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 36685626 || all params: 42977082 || trainable%: 85.36\n",
      "\u001b[32m2024-09-22 06:37:20.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m630\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): BertForMaskedLM(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 512)\n",
      "        (token_type_embeddings): Embedding(1, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-7): 8 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (key): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (value): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mTotal params  before LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m632\u001b[0m - \u001b[1mTotal params  after  LoRA: 42.98M\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mTrainable params: 36.69M\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mIn total, added 1.57M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mSaving model to checkpoint/smart-surf-738 every 600000 update steps\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m657\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-09-22 06:37:20.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[1mScheduler will run for 62500 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 06:37:20.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1mFull training set size: 4000000\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m763\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mTrain set size after shard: 4000000\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 06:37:20.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m772\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-09-22 06:37:20.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m871\u001b[0m - \u001b[1mStarting training at update step 0 with 62500 update steps\u001b[0m\n",
      "Update steps:   0%|                                   | 0/62500 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 06:37:36.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m904\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Update steps:   5%|‚ñà                     | 3000/62500 [05:42<1:48:30,  9.14it/s]\u001b[32m2024-09-22 06:43:02.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 3000\u001b[0m\n",
      "\u001b[32m2024-09-22 06:43:16.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10294272.0 tokens, eval loss: 6.6835\u001b[0m\n",
      "\u001b[32m2024-09-22 06:43:16.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 799.11\u001b[0m\n",
      "\u001b[32m2024-09-22 06:43:16.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.94 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 06:43:16.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 3000: 6.6835036277771, Perplexity: 799.1140110601193\u001b[0m\n",
      "Update steps:  10%|‚ñà‚ñà                    | 5999/62500 [11:22<1:44:36,  9.00it/s]\u001b[32m2024-09-22 06:48:42.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 6000\u001b[0m\n",
      "\u001b[32m2024-09-22 06:48:56.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10057856.0 tokens, eval loss: 6.4036\u001b[0m\n",
      "\u001b[32m2024-09-22 06:48:56.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 604.02\u001b[0m\n",
      "\u001b[32m2024-09-22 06:48:56.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.64 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 06:48:56.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 6000: 6.403604507446289, Perplexity: 604.0183072223705\u001b[0m\n",
      "Update steps:  14%|‚ñà‚ñà‚ñà‚ñè                  | 9000/62500 [17:03<1:40:39,  8.86it/s]\u001b[32m2024-09-22 06:54:23.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 9000\u001b[0m\n",
      "\u001b[32m2024-09-22 06:54:40.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11892160.0 tokens, eval loss: 5.6521\u001b[0m\n",
      "\u001b[32m2024-09-22 06:54:40.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 284.89\u001b[0m\n",
      "\u001b[32m2024-09-22 06:54:40.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 16.10 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 06:54:40.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 9000: 5.652088165283203, Perplexity: 284.88573364193974\u001b[0m\n",
      "Update steps:  19%|‚ñà‚ñà‚ñà‚ñà                 | 11999/62500 [22:46<1:26:45,  9.70it/s]\u001b[32m2024-09-22 07:00:06.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 12000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:00:18.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8459968.0 tokens, eval loss: 4.3880\u001b[0m\n",
      "\u001b[32m2024-09-22 07:00:18.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 80.48\u001b[0m\n",
      "\u001b[32m2024-09-22 07:00:18.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.45 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:00:18.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 12000: 4.387988090515137, Perplexity: 80.47834085459759\u001b[0m\n",
      "Update steps:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà                | 15000/62500 [28:24<1:25:32,  9.26it/s]\u001b[32m2024-09-22 07:05:44.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 15000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:05:55.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8081344.0 tokens, eval loss: 3.9517\u001b[0m\n",
      "\u001b[32m2024-09-22 07:05:55.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 52.03\u001b[0m\n",
      "\u001b[32m2024-09-22 07:05:55.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.94 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:05:55.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 15000: 3.9517462253570557, Perplexity: 52.02613691877207\u001b[0m\n",
      "Update steps:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 18000/62500 [34:01<1:15:08,  9.87it/s]\u001b[32m2024-09-22 07:11:21.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 18000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:11:34.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9698816.0 tokens, eval loss: 3.7350\u001b[0m\n",
      "\u001b[32m2024-09-22 07:11:34.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 41.89\u001b[0m\n",
      "\u001b[32m2024-09-22 07:11:34.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.12 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:11:34.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 18000: 3.7349743843078613, Perplexity: 41.88695258607819\u001b[0m\n",
      "Update steps:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 21000/62500 [39:41<1:12:55,  9.48it/s]\u001b[32m2024-09-22 07:17:01.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 21000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:17:16.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10771584.0 tokens, eval loss: 3.5898\u001b[0m\n",
      "\u001b[32m2024-09-22 07:17:16.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 36.23\u001b[0m\n",
      "\u001b[32m2024-09-22 07:17:16.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.57 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:17:16.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 21000: 3.5897860527038574, Perplexity: 36.22632457312388\u001b[0m\n",
      "Update steps:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 23999/62500 [45:24<1:05:44,  9.76it/s]\u001b[32m2024-09-22 07:22:45.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 24000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:22:58.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9764544.0 tokens, eval loss: 3.4811\u001b[0m\n",
      "\u001b[32m2024-09-22 07:22:58.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 32.49\u001b[0m\n",
      "\u001b[32m2024-09-22 07:22:58.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.20 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:22:58.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 24000: 3.4810566902160645, Perplexity: 32.494040075151304\u001b[0m\n",
      "Update steps:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 27000/62500 [51:06<1:02:13,  9.51it/s]\u001b[32m2024-09-22 07:28:26.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 27000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:28:41.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10676672.0 tokens, eval loss: 3.4145\u001b[0m\n",
      "\u001b[32m2024-09-22 07:28:41.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 30.40\u001b[0m\n",
      "\u001b[32m2024-09-22 07:28:41.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.44 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:28:41.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 27000: 3.4144530296325684, Perplexity: 30.40031680675068\u001b[0m\n",
      "Update steps:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 30000/62500 [56:47<58:42,  9.23it/s]\u001b[32m2024-09-22 07:34:07.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 30000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:34:18.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8097664.0 tokens, eval loss: 3.3550\u001b[0m\n",
      "\u001b[32m2024-09-22 07:34:18.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 28.65\u001b[0m\n",
      "\u001b[32m2024-09-22 07:34:18.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.94 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:34:18.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 30000: 3.355034828186035, Perplexity: 28.646601882512254\u001b[0m\n",
      "Update steps:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 32999/62500 [1:02:25<55:54,  8.79it/s]\u001b[32m2024-09-22 07:39:45.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 33000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:40:01.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11561856.0 tokens, eval loss: 3.3094\u001b[0m\n",
      "\u001b[32m2024-09-22 07:40:01.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 27.37\u001b[0m\n",
      "\u001b[32m2024-09-22 07:40:01.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.65 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:40:01.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 33000: 3.309417963027954, Perplexity: 27.369190954083546\u001b[0m\n",
      "Update steps:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 36000/62500 [1:08:09<47:23,  9.32it/s]\u001b[32m2024-09-22 07:45:29.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 36000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:45:43.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9914560.0 tokens, eval loss: 3.2835\u001b[0m\n",
      "\u001b[32m2024-09-22 07:45:43.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 26.67\u001b[0m\n",
      "\u001b[32m2024-09-22 07:45:43.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.43 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:45:43.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 36000: 3.2835121154785156, Perplexity: 26.66927398006225\u001b[0m\n",
      "Update steps:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 39000/62500 [1:13:49<42:30,  9.21it/s]\u001b[32m2024-09-22 07:51:10.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 39000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:51:26.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11786304.0 tokens, eval loss: 3.2501\u001b[0m\n",
      "\u001b[32m2024-09-22 07:51:26.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.79\u001b[0m\n",
      "\u001b[32m2024-09-22 07:51:26.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.96 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:51:26.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 39000: 3.2501046657562256, Perplexity: 25.7930394238943\u001b[0m\n",
      "Update steps:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 42000/62500 [1:19:32<36:37,  9.33it/s]\u001b[32m2024-09-22 07:56:53.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 42000\u001b[0m\n",
      "\u001b[32m2024-09-22 07:57:06.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10233600.0 tokens, eval loss: 3.2289\u001b[0m\n",
      "\u001b[32m2024-09-22 07:57:06.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.25\u001b[0m\n",
      "\u001b[32m2024-09-22 07:57:06.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.86 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 07:57:06.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 42000: 3.2289083003997803, Perplexity: 25.252074238322542\u001b[0m\n",
      "Update steps:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 45000/62500 [1:25:14<32:05,  9.09it/s]\u001b[32m2024-09-22 08:02:34.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 45000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:02:49.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10827456.0 tokens, eval loss: 3.2160\u001b[0m\n",
      "\u001b[32m2024-09-22 08:02:49.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.93\u001b[0m\n",
      "\u001b[32m2024-09-22 08:02:49.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.65 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:02:49.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 45000: 3.216012716293335, Perplexity: 24.928524655290456\u001b[0m\n",
      "Update steps:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 48000/62500 [1:30:56<26:19,  9.18it/s]\u001b[32m2024-09-22 08:08:16.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 48000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:08:28.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8874624.0 tokens, eval loss: 3.2080\u001b[0m\n",
      "\u001b[32m2024-09-22 08:08:28.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.73\u001b[0m\n",
      "\u001b[32m2024-09-22 08:08:28.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.01 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:08:28.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 48000: 3.208022356033325, Perplexity: 24.73013043872901\u001b[0m\n",
      "Update steps:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 51000/62500 [1:36:36<18:54, 10.14it/s]\u001b[32m2024-09-22 08:13:56.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 51000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:14:07.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8136320.0 tokens, eval loss: 3.1912\u001b[0m\n",
      "\u001b[32m2024-09-22 08:14:07.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.32\u001b[0m\n",
      "\u001b[32m2024-09-22 08:14:07.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.99 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:14:07.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 51000: 3.191176414489746, Perplexity: 24.317017514661714\u001b[0m\n",
      "Update steps:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 54000/62500 [1:42:15<15:10,  9.33it/s]\u001b[32m2024-09-22 08:19:35.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 54000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:19:50.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11175808.0 tokens, eval loss: 3.1958\u001b[0m\n",
      "\u001b[32m2024-09-22 08:19:50.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.43\u001b[0m\n",
      "\u001b[32m2024-09-22 08:19:50.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.11 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:19:50.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 54000: 3.1958136558532715, Perplexity: 24.43004325539619\u001b[0m\n",
      "Update steps:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 57000/62500 [1:47:58<09:51,  9.30it/s]\u001b[32m2024-09-22 08:25:18.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 57000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:25:31.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9911104.0 tokens, eval loss: 3.1799\u001b[0m\n",
      "\u001b[32m2024-09-22 08:25:31.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.04\u001b[0m\n",
      "\u001b[32m2024-09-22 08:25:31.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.40 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:25:31.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 57000: 3.179917812347412, Perplexity: 24.044777287051094\u001b[0m\n",
      "Update steps:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 60000/62500 [1:53:39<04:19,  9.63it/s]\u001b[32m2024-09-22 08:30:59.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 60000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:31:13.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10596096.0 tokens, eval loss: 3.1805\u001b[0m\n",
      "\u001b[32m2024-09-22 08:31:13.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.06\u001b[0m\n",
      "\u001b[32m2024-09-22 08:31:13.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.35 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:31:13.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 60000: 3.1804609298706055, Perplexity: 24.05783997390294\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:26<00:00,  9.26it/s]Warning: reached the end of the dataset. Training stopped, global_rank=0, update_step=62500\n",
      "\u001b[32m2024-09-22 08:35:46.307\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1081\u001b[0m - \u001b[33m\u001b[1mReached the end of the dataset. Training stopped\u001b[0m\n",
      "\u001b[32m2024-09-22 08:35:46.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1087\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:26<00:00,  8.80it/s]\n",
      "\u001b[32m2024-09-22 08:35:46.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1092\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/smart-surf-738/model_62500, update step 62500\u001b[0m\n",
      "\u001b[32m2024-09-22 08:35:46.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m243\u001b[0m - \u001b[1mSaving took 0.19 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:35:46.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1112\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
      "\u001b[32m2024-09-22 08:36:42.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 41385760.0 tokens, eval loss: 3.1814\u001b[0m\n",
      "\u001b[32m2024-09-22 08:36:42.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 24.08\u001b[0m\n",
      "\u001b[32m2024-09-22 08:36:42.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 55.93 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:36:42.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1131\u001b[0m - \u001b[1mFinal eval loss: 3.181391954421997\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm ‚ñÜ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ‚ñÑ‚ñà‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñÇ‚ñá‚ñà‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñà‚ñÇ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÉ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 3.18139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 41385760.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm 1.85938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 3.53125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity 24.08025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 9.25331\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 592.21193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 242806.8924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 1652867648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 62500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33msmart-surf-738\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/mibktbuq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240922_063718-mibktbuq/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "\u001b[32m2024-09-22 08:36:49.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1154\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
      "Rank 0 finished successfully\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path bert_c4 \\\n",
    "    --dataset_path /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM \\\n",
    "    --batch_size 64\\\n",
    "    --lr 6e-4 \\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.02 \\\n",
    "    --num_training_steps 62500 \\\n",
    "    --save_every 600000\\\n",
    "    --eval_every 3000 \\\n",
    "    --warmup_steps 9000 \\\n",
    "    --seed 42 \\\n",
    "    --tags relora_bert_medium \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 62500 \\\n",
    "    --restart_warmup_steps 500 \\\n",
    "    --scheduler linear \\\n",
    "    --reset_optimizer_on_relora False \\\n",
    "    --relora 62500\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.9\\\n",
    "    --lora_alpha 128 \\\n",
    "    --min_lr_ratio 0.000000000000000000000000000000000000000000001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df08bf93-8ff1-463d-acaf-ae6ab47931c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-22 08:36:58.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-09-22 08:36:58.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240922_083659-mn2c40ra\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclassic-fog-739\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/mn2c40ra\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_name_or_path             bert_c4\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdataset_path                   /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mbatch_size                     64\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtotal_batch_size               64\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_r                         64\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_alpha                     64.0\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrelora                         62500\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mreset_optimizer_on_relora      False\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlr                             0.0006\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mscheduler                      linear\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcycle_length                   62500\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrestart_warmup_steps           500\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmin_lr_ratio                   1e-45\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mweight_decay                   0.02\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmup_steps                   9000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1meval_every                     3000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mnum_training_steps             62500\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_every                     600000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_dir                       checkpoint/classic-fog-739\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtags                           ['relora_bert_medium']\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrun_name                       classic-fog-739\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m473\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:00.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1mUsing HuggingFace model bert_c4 revision None\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m565\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 36685626 || all params: 42977082 || trainable%: 85.36\n",
      "\u001b[32m2024-09-22 08:37:01.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m630\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): BertForMaskedLM(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 512)\n",
      "        (token_type_embeddings): Embedding(1, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-7): 8 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (key): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (value): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mTotal params  before LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m632\u001b[0m - \u001b[1mTotal params  after  LoRA: 42.98M\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mTrainable params: 36.69M\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mIn total, added 1.57M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mSaving model to checkpoint/classic-fog-739 every 600000 update steps\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m657\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-09-22 08:37:01.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[1mScheduler will run for 62500 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 08:37:01.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1mFull training set size: 4000000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m763\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mTrain set size after shard: 4000000\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 08:37:01.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m772\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-09-22 08:37:01.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m871\u001b[0m - \u001b[1mStarting training at update step 0 with 62500 update steps\u001b[0m\n",
      "Update steps:   0%|                                   | 0/62500 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 08:37:17.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m904\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Update steps:   5%|‚ñà                     | 3000/62500 [05:42<1:48:30,  9.14it/s]\u001b[32m2024-09-22 08:42:43.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 3000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:42:57.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10294272.0 tokens, eval loss: 6.6872\u001b[0m\n",
      "\u001b[32m2024-09-22 08:42:57.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 802.06\u001b[0m\n",
      "\u001b[32m2024-09-22 08:42:57.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.95 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:42:57.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 3000: 6.687180519104004, Perplexity: 802.0576748861779\u001b[0m\n",
      "Update steps:  10%|‚ñà‚ñà                    | 5999/62500 [11:23<1:44:36,  9.00it/s]\u001b[32m2024-09-22 08:48:24.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 6000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:48:38.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10057856.0 tokens, eval loss: 6.4250\u001b[0m\n",
      "\u001b[32m2024-09-22 08:48:38.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 617.09\u001b[0m\n",
      "\u001b[32m2024-09-22 08:48:38.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.65 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:48:38.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 6000: 6.425016403198242, Perplexity: 617.0909397248427\u001b[0m\n",
      "Update steps:  14%|‚ñà‚ñà‚ñà‚ñè                  | 9000/62500 [17:04<1:40:41,  8.86it/s]\u001b[32m2024-09-22 08:54:05.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 9000\u001b[0m\n",
      "\u001b[32m2024-09-22 08:54:22.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11892160.0 tokens, eval loss: 6.0879\u001b[0m\n",
      "\u001b[32m2024-09-22 08:54:22.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 440.49\u001b[0m\n",
      "\u001b[32m2024-09-22 08:54:22.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 16.13 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 08:54:22.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 9000: 6.087894916534424, Perplexity: 440.4931595982944\u001b[0m\n",
      "Update steps:  19%|‚ñà‚ñà‚ñà‚ñà                 | 11999/62500 [22:47<1:26:48,  9.70it/s]\u001b[32m2024-09-22 08:59:49.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 12000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:00:00.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8459968.0 tokens, eval loss: 4.6055\u001b[0m\n",
      "\u001b[32m2024-09-22 09:00:00.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 100.03\u001b[0m\n",
      "\u001b[32m2024-09-22 09:00:00.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.44 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:00:00.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 12000: 4.60546875, Perplexity: 100.02986085865793\u001b[0m\n",
      "Update steps:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà                | 15000/62500 [28:25<1:25:45,  9.23it/s]\u001b[32m2024-09-22 09:05:27.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 15000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:05:38.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8081344.0 tokens, eval loss: 4.0926\u001b[0m\n",
      "\u001b[32m2024-09-22 09:05:38.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 59.90\u001b[0m\n",
      "\u001b[32m2024-09-22 09:05:38.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.96 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:05:38.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 15000: 4.092626571655273, Perplexity: 59.89700906005342\u001b[0m\n",
      "Update steps:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 18000/62500 [34:03<1:15:09,  9.87it/s]\u001b[32m2024-09-22 09:11:04.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 18000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:11:17.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9698816.0 tokens, eval loss: 3.8560\u001b[0m\n",
      "\u001b[32m2024-09-22 09:11:17.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 47.28\u001b[0m\n",
      "\u001b[32m2024-09-22 09:11:17.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.11 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:11:17.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 18000: 3.8560359477996826, Perplexity: 47.27756867441935\u001b[0m\n",
      "Update steps:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 21000/62500 [39:43<1:12:54,  9.49it/s]\u001b[32m2024-09-22 09:16:44.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 21000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:16:59.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10771584.0 tokens, eval loss: 3.6952\u001b[0m\n",
      "\u001b[32m2024-09-22 09:16:59.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 40.25\u001b[0m\n",
      "\u001b[32m2024-09-22 09:16:59.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.59 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:16:59.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 21000: 3.6951584815979004, Perplexity: 40.25195127606279\u001b[0m\n",
      "Update steps:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 23999/62500 [45:26<1:05:45,  9.76it/s]\u001b[32m2024-09-22 09:22:27.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 24000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:22:41.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9764544.0 tokens, eval loss: 3.5814\u001b[0m\n",
      "\u001b[32m2024-09-22 09:22:41.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 35.92\u001b[0m\n",
      "\u001b[32m2024-09-22 09:22:41.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.22 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:22:41.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 24000: 3.5814011096954346, Perplexity: 35.923838841228424\u001b[0m\n",
      "Update steps:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 27000/62500 [51:07<1:02:17,  9.50it/s]\u001b[32m2024-09-22 09:28:09.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 27000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:28:24.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10676672.0 tokens, eval loss: 3.5060\u001b[0m\n",
      "\u001b[32m2024-09-22 09:28:24.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 33.32\u001b[0m\n",
      "\u001b[32m2024-09-22 09:28:24.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.48 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:28:24.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 27000: 3.5060155391693115, Perplexity: 33.315259629964196\u001b[0m\n",
      "Update steps:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 30000/62500 [56:48<58:44,  9.22it/s]\u001b[32m2024-09-22 09:33:50.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 30000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:34:01.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8097664.0 tokens, eval loss: 3.4358\u001b[0m\n",
      "\u001b[32m2024-09-22 09:34:01.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 31.06\u001b[0m\n",
      "\u001b[32m2024-09-22 09:34:01.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.97 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:34:01.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 30000: 3.43581485748291, Perplexity: 31.05670904937722\u001b[0m\n",
      "Update steps:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 32999/62500 [1:02:27<55:55,  8.79it/s]\u001b[32m2024-09-22 09:39:28.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 33000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:39:44.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11561856.0 tokens, eval loss: 3.3864\u001b[0m\n",
      "\u001b[32m2024-09-22 09:39:44.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 29.56\u001b[0m\n",
      "\u001b[32m2024-09-22 09:39:44.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.65 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:39:44.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 33000: 3.3863961696624756, Perplexity: 29.559233625032668\u001b[0m\n",
      "Update steps:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 36000/62500 [1:08:11<47:24,  9.32it/s]\u001b[32m2024-09-22 09:45:12.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 36000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:45:26.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9914560.0 tokens, eval loss: 3.3559\u001b[0m\n",
      "\u001b[32m2024-09-22 09:45:26.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 28.67\u001b[0m\n",
      "\u001b[32m2024-09-22 09:45:26.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.45 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:45:26.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 36000: 3.355898141860962, Perplexity: 28.671343564036924\u001b[0m\n",
      "Update steps:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 39000/62500 [1:13:51<42:30,  9.21it/s]\u001b[32m2024-09-22 09:50:53.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 39000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:51:09.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11786304.0 tokens, eval loss: 3.3156\u001b[0m\n",
      "\u001b[32m2024-09-22 09:51:09.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 27.54\u001b[0m\n",
      "\u001b[32m2024-09-22 09:51:09.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.98 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:51:09.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 39000: 3.3155691623687744, Perplexity: 27.538063154250917\u001b[0m\n",
      "Update steps:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 42000/62500 [1:19:34<36:36,  9.33it/s]\u001b[32m2024-09-22 09:56:36.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 42000\u001b[0m\n",
      "\u001b[32m2024-09-22 09:56:50.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10233600.0 tokens, eval loss: 3.2932\u001b[0m\n",
      "\u001b[32m2024-09-22 09:56:50.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 26.93\u001b[0m\n",
      "\u001b[32m2024-09-22 09:56:50.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.88 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 09:56:50.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 42000: 3.2932441234588623, Perplexity: 26.930086624129423\u001b[0m\n",
      "Update steps:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 45000/62500 [1:25:16<32:07,  9.08it/s]\u001b[32m2024-09-22 10:02:18.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 45000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:02:32.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10827456.0 tokens, eval loss: 3.2812\u001b[0m\n",
      "\u001b[32m2024-09-22 10:02:32.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 26.61\u001b[0m\n",
      "\u001b[32m2024-09-22 10:02:32.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.70 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:02:32.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 45000: 3.2811739444732666, Perplexity: 26.606989501168336\u001b[0m\n",
      "Update steps:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 48000/62500 [1:30:58<26:17,  9.19it/s]\u001b[32m2024-09-22 10:08:00.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 48000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:08:12.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8874624.0 tokens, eval loss: 3.2684\u001b[0m\n",
      "\u001b[32m2024-09-22 10:08:12.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 26.27\u001b[0m\n",
      "\u001b[32m2024-09-22 10:08:12.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.03 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:08:12.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 48000: 3.268423557281494, Perplexity: 26.269893700867943\u001b[0m\n",
      "Update steps:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 51000/62500 [1:36:39<18:56, 10.12it/s]\u001b[32m2024-09-22 10:13:40.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 51000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:13:51.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8136320.0 tokens, eval loss: 3.2551\u001b[0m\n",
      "\u001b[32m2024-09-22 10:13:51.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.92\u001b[0m\n",
      "\u001b[32m2024-09-22 10:13:51.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.05 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:13:51.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 51000: 3.2550551891326904, Perplexity: 25.92104505451907\u001b[0m\n",
      "Update steps:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 54000/62500 [1:42:18<15:12,  9.31it/s]\u001b[32m2024-09-22 10:19:19.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 54000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:19:35.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11175808.0 tokens, eval loss: 3.2582\u001b[0m\n",
      "\u001b[32m2024-09-22 10:19:35.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 26.00\u001b[0m\n",
      "\u001b[32m2024-09-22 10:19:35.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.17 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:19:35.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 54000: 3.258165121078491, Perplexity: 26.0017832206309\u001b[0m\n",
      "Update steps:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 57000/62500 [1:48:01<09:50,  9.31it/s]\u001b[32m2024-09-22 10:25:03.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 57000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:25:16.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9911104.0 tokens, eval loss: 3.2416\u001b[0m\n",
      "\u001b[32m2024-09-22 10:25:16.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.57\u001b[0m\n",
      "\u001b[32m2024-09-22 10:25:16.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.40 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:25:16.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 57000: 3.241580009460449, Perplexity: 25.5740971576384\u001b[0m\n",
      "Update steps:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 60000/62500 [1:53:42<04:20,  9.61it/s]\u001b[32m2024-09-22 10:30:44.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 60000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:30:58.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10596096.0 tokens, eval loss: 3.2408\u001b[0m\n",
      "\u001b[32m2024-09-22 10:30:58.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.55\u001b[0m\n",
      "\u001b[32m2024-09-22 10:30:58.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.39 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:30:58.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 60000: 3.2407565116882324, Perplexity: 25.553045614741638\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:30<00:00,  9.25it/s]Warning: reached the end of the dataset. Training stopped, global_rank=0, update_step=62500\n",
      "\u001b[32m2024-09-22 10:35:32.355\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1081\u001b[0m - \u001b[33m\u001b[1mReached the end of the dataset. Training stopped\u001b[0m\n",
      "\u001b[32m2024-09-22 10:35:32.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1087\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:30<00:00,  8.79it/s]\n",
      "\u001b[32m2024-09-22 10:35:32.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1092\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/classic-fog-739/model_62500, update step 62500\u001b[0m\n",
      "\u001b[32m2024-09-22 10:35:32.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m243\u001b[0m - \u001b[1mSaving took 0.19 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:35:32.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1112\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:28.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 41385760.0 tokens, eval loss: 3.2442\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:28.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.64\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:28.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 56.05 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:28.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1131\u001b[0m - \u001b[1mFinal eval loss: 3.244231939315796\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss ‚ñà‚ñá‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm ‚ñá‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity ‚ñà‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ‚ñÑ‚ñà‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñÅ‚ñá‚ñá‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñà‚ñÇ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 3.24423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 41385760.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm 1.9375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 3.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity 25.64201\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 9.22155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 590.17946\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 241973.57946\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 1652867648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 62500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mclassic-fog-739\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/mn2c40ra\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240922_083659-mn2c40ra/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "\u001b[32m2024-09-22 10:36:35.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1154\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
      "Rank 0 finished successfully\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path bert_c4 \\\n",
    "    --dataset_path /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM \\\n",
    "    --batch_size 64\\\n",
    "    --lr 6e-4 \\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.02 \\\n",
    "    --num_training_steps 62500 \\\n",
    "    --save_every 600000\\\n",
    "    --eval_every 3000 \\\n",
    "    --warmup_steps 9000 \\\n",
    "    --seed 42 \\\n",
    "    --tags relora_bert_medium \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 62500 \\\n",
    "    --restart_warmup_steps 500 \\\n",
    "    --scheduler linear \\\n",
    "    --reset_optimizer_on_relora False \\\n",
    "    --relora 62500\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.9\\\n",
    "    --lora_alpha 64 \\\n",
    "    --min_lr_ratio 0.000000000000000000000000000000000000000000001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "143aa375-e39e-46a3-8c81-c0b3e4c816d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-22 10:36:44.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:44.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240922_103644-0yhz6j2s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexpert-feather-740\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/0yhz6j2s\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_name_or_path             bert_c4\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdataset_path                   /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mbatch_size                     64\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtotal_batch_size               64\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_r                         64\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_alpha                     64.0\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrelora                         62500\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mreset_optimizer_on_relora      False\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlr                             0.0006\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mscheduler                      linear\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcycle_length                   62500\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrestart_warmup_steps           500\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmin_lr_ratio                   1e-45\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mweight_decay                   0.02\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmup_steps                   9000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1meval_every                     3000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mnum_training_steps             62500\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_every                     600000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_dir                       checkpoint/expert-feather-740\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtags                           ['relora_bert_medium']\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrun_name                       expert-feather-740\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:45.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:46.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:46.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m473\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:46.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1mUsing HuggingFace model bert_c4 revision None\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m565\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 36685626 || all params: 42977082 || trainable%: 85.36\n",
      "\u001b[32m2024-09-22 10:36:47.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m630\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): BertForMaskedLM(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 512)\n",
      "        (token_type_embeddings): Embedding(1, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-7): 8 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (key): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (value): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mTotal params  before LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m632\u001b[0m - \u001b[1mTotal params  after  LoRA: 42.98M\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mTrainable params: 36.69M\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mIn total, added 1.57M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mSaving model to checkpoint/expert-feather-740 every 600000 update steps\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m657\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-09-22 10:36:47.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[1mScheduler will run for 62500 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 10:36:47.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1mFull training set size: 4000000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m763\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mTrain set size after shard: 4000000\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 10:36:47.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m772\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-09-22 10:36:47.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m871\u001b[0m - \u001b[1mStarting training at update step 0 with 62500 update steps\u001b[0m\n",
      "Update steps:   0%|                                   | 0/62500 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 10:37:03.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m904\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Update steps:   5%|‚ñà                     | 3000/62500 [05:42<1:49:23,  9.07it/s]\u001b[32m2024-09-22 10:42:30.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 3000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:42:44.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10294272.0 tokens, eval loss: 6.6870\u001b[0m\n",
      "\u001b[32m2024-09-22 10:42:44.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 801.93\u001b[0m\n",
      "\u001b[32m2024-09-22 10:42:44.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.00 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:42:44.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 3000: 6.687020301818848, Perplexity: 801.9291816766603\u001b[0m\n",
      "Update steps:  10%|‚ñà‚ñà                    | 5999/62500 [11:23<1:44:30,  9.01it/s]\u001b[32m2024-09-22 10:48:10.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 6000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:48:24.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10057856.0 tokens, eval loss: 6.4245\u001b[0m\n",
      "\u001b[32m2024-09-22 10:48:24.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 616.78\u001b[0m\n",
      "\u001b[32m2024-09-22 10:48:24.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.67 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:48:24.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 6000: 6.424520492553711, Perplexity: 616.7849936264229\u001b[0m\n",
      "Update steps:  14%|‚ñà‚ñà‚ñà‚ñè                  | 9000/62500 [17:04<1:40:34,  8.87it/s]\u001b[32m2024-09-22 10:53:51.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 9000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:54:08.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11892160.0 tokens, eval loss: 6.0906\u001b[0m\n",
      "\u001b[32m2024-09-22 10:54:08.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 441.68\u001b[0m\n",
      "\u001b[32m2024-09-22 10:54:08.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 16.16 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:54:08.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 9000: 6.090575695037842, Perplexity: 441.675608425\u001b[0m\n",
      "Update steps:  19%|‚ñà‚ñà‚ñà‚ñà                 | 11999/62500 [22:47<1:26:43,  9.71it/s]\u001b[32m2024-09-22 10:59:34.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 12000\u001b[0m\n",
      "\u001b[32m2024-09-22 10:59:46.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8459968.0 tokens, eval loss: 4.5931\u001b[0m\n",
      "\u001b[32m2024-09-22 10:59:46.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 98.80\u001b[0m\n",
      "\u001b[32m2024-09-22 10:59:46.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.48 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 10:59:46.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 12000: 4.593066215515137, Perplexity: 98.79689879189041\u001b[0m\n",
      "Update steps:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà                | 15000/62500 [28:25<1:25:30,  9.26it/s]\u001b[32m2024-09-22 11:05:12.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 15000\u001b[0m\n",
      "\u001b[32m2024-09-22 11:05:23.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8081344.0 tokens, eval loss: 4.0727\u001b[0m\n",
      "\u001b[32m2024-09-22 11:05:23.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 58.71\u001b[0m\n",
      "\u001b[32m2024-09-22 11:05:23.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.97 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 11:05:23.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 15000: 4.072661399841309, Perplexity: 58.713013647859974\u001b[0m\n",
      "Update steps:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 18000/62500 [34:02<1:15:13,  9.86it/s]\u001b[32m2024-09-22 11:10:49.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 18000\u001b[0m\n",
      "\u001b[32m2024-09-22 11:11:02.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9698816.0 tokens, eval loss: 3.8280\u001b[0m\n",
      "\u001b[32m2024-09-22 11:11:02.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 45.97\u001b[0m\n",
      "\u001b[32m2024-09-22 11:11:02.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.17 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 11:11:02.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 18000: 3.827953815460205, Perplexity: 45.96838214383576\u001b[0m\n",
      "Update steps:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 21000/62500 [39:42<1:13:00,  9.47it/s]\u001b[32m2024-09-22 11:16:29.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 21000\u001b[0m\n",
      "\u001b[32m2024-09-22 11:16:44.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10771584.0 tokens, eval loss: 3.6698\u001b[0m\n",
      "\u001b[32m2024-09-22 11:16:44.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 39.24\u001b[0m\n",
      "\u001b[32m2024-09-22 11:16:44.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.65 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 11:16:44.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 21000: 3.6697967052459717, Perplexity: 39.24392696481876\u001b[0m\n",
      "Update steps:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 23999/62500 [45:25<1:05:48,  9.75it/s]\u001b[32m2024-09-22 11:22:13.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 24000\u001b[0m\n",
      "\u001b[32m2024-09-22 11:22:26.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9764544.0 tokens, eval loss: 3.5573\u001b[0m\n",
      "\u001b[32m2024-09-22 11:22:26.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 35.07\u001b[0m\n",
      "\u001b[32m2024-09-22 11:22:26.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.26 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 11:22:26.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 24000: 3.5573337078094482, Perplexity: 35.069566665740886\u001b[0m\n",
      "Update steps:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 27000/62500 [51:07<1:02:16,  9.50it/s]\u001b[32m2024-09-22 11:27:55.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 27000\u001b[0m\n",
      "\u001b[32m2024-09-22 11:28:09.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10676672.0 tokens, eval loss: 3.4895\u001b[0m\n",
      "\u001b[32m2024-09-22 11:28:09.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 32.77\u001b[0m\n",
      "\u001b[32m2024-09-22 11:28:09.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.49 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 11:28:09.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 27000: 3.4895312786102295, Perplexity: 32.77058383222755\u001b[0m\n",
      "Update steps:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 30000/62500 [56:48<58:36,  9.24it/s]\u001b[32m2024-09-22 11:33:35.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 30000\u001b[0m\n",
      "\u001b[32m2024-09-22 11:33:46.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8097664.0 tokens, eval loss: 3.4156\u001b[0m\n",
      "\u001b[32m2024-09-22 11:33:46.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 30.44\u001b[0m\n",
      "\u001b[32m2024-09-22 11:33:46.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.96 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 11:33:46.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 30000: 3.4155943393707275, Perplexity: 30.435032791442662\u001b[0m\n",
      "Update steps:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 32999/62500 [1:02:26<56:04,  8.77it/s]\u001b[32m2024-09-22 11:39:14.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 33000\u001b[0m\n",
      "\u001b[32m2024-09-22 11:39:30.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11561856.0 tokens, eval loss: 3.3698\u001b[0m\n",
      "\u001b[32m2024-09-22 11:39:30.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 29.07\u001b[0m\n",
      "\u001b[32m2024-09-22 11:39:30.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.64 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 11:39:30.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 33000: 3.3698036670684814, Perplexity: 29.072818545738365\u001b[0m\n",
      "Update steps:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 36000/62500 [1:08:10<47:24,  9.32it/s]\u001b[32m2024-09-22 11:44:58.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 36000\u001b[0m\n",
      "\u001b[32m2024-09-22 11:45:11.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9914560.0 tokens, eval loss: 3.3426\u001b[0m\n",
      "\u001b[32m2024-09-22 11:45:11.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 28.29\u001b[0m\n",
      "\u001b[32m2024-09-22 11:45:11.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.43 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 11:45:11.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 36000: 3.3425769805908203, Perplexity: 28.291940626954236\u001b[0m\n",
      "Update steps:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 39000/62500 [1:13:50<42:27,  9.22it/s]\u001b[32m2024-09-22 11:50:38.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 39000\u001b[0m\n",
      "\u001b[32m2024-09-22 11:50:54.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11786304.0 tokens, eval loss: 3.3001\u001b[0m\n",
      "\u001b[32m2024-09-22 11:50:54.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 27.12\u001b[0m\n",
      "\u001b[32m2024-09-22 11:50:54.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.95 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 11:50:54.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 39000: 3.3001186847686768, Perplexity: 27.11585696889928\u001b[0m\n",
      "Update steps:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 42000/62500 [1:19:33<36:38,  9.33it/s]\u001b[32m2024-09-22 11:56:21.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 42000\u001b[0m\n",
      "\u001b[32m2024-09-22 11:56:35.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10233600.0 tokens, eval loss: 3.2775\u001b[0m\n",
      "\u001b[32m2024-09-22 11:56:35.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 26.51\u001b[0m\n",
      "\u001b[32m2024-09-22 11:56:35.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.91 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 11:56:35.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 42000: 3.277456045150757, Perplexity: 26.50825105646284\u001b[0m\n",
      "Update steps:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 45000/62500 [1:25:15<32:12,  9.06it/s]\u001b[32m2024-09-22 12:02:03.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 45000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:02:18.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10827456.0 tokens, eval loss: 3.2655\u001b[0m\n",
      "\u001b[32m2024-09-22 12:02:18.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 26.19\u001b[0m\n",
      "\u001b[32m2024-09-22 12:02:18.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.73 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:02:18.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 45000: 3.2655110359191895, Perplexity: 26.19349338705897\u001b[0m\n",
      "Update steps:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 48000/62500 [1:30:58<26:18,  9.19it/s]\u001b[32m2024-09-22 12:07:45.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 48000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:07:57.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8874624.0 tokens, eval loss: 3.2543\u001b[0m\n",
      "\u001b[32m2024-09-22 12:07:57.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.90\u001b[0m\n",
      "\u001b[32m2024-09-22 12:07:57.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 12.01 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:07:57.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 48000: 3.25429105758667, Perplexity: 25.901245531970385\u001b[0m\n",
      "Update steps:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 51000/62500 [1:36:38<18:54, 10.14it/s]\u001b[32m2024-09-22 12:13:25.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 51000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:13:36.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8136320.0 tokens, eval loss: 3.2409\u001b[0m\n",
      "\u001b[32m2024-09-22 12:13:36.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.56\u001b[0m\n",
      "\u001b[32m2024-09-22 12:13:36.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.01 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:13:36.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 51000: 3.2408599853515625, Perplexity: 25.555689818781016\u001b[0m\n",
      "Update steps:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 54000/62500 [1:42:17<15:11,  9.32it/s]\u001b[32m2024-09-22 12:19:04.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 54000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:19:19.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11175808.0 tokens, eval loss: 3.2424\u001b[0m\n",
      "\u001b[32m2024-09-22 12:19:19.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.59\u001b[0m\n",
      "\u001b[32m2024-09-22 12:19:19.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.17 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:19:19.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 54000: 3.242391586303711, Perplexity: 25.59486092723647\u001b[0m\n",
      "Update steps:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 57000/62500 [1:48:00<09:51,  9.30it/s]\u001b[32m2024-09-22 12:24:47.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 57000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:25:01.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9911104.0 tokens, eval loss: 3.2297\u001b[0m\n",
      "\u001b[32m2024-09-22 12:25:01.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.27\u001b[0m\n",
      "\u001b[32m2024-09-22 12:25:01.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.48 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:25:01.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 57000: 3.2296833992004395, Perplexity: 25.27165467818626\u001b[0m\n",
      "Update steps:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 60000/62500 [1:53:41<04:19,  9.64it/s]\u001b[32m2024-09-22 12:30:28.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 60000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:30:43.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10596096.0 tokens, eval loss: 3.2266\u001b[0m\n",
      "\u001b[32m2024-09-22 12:30:43.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.19\u001b[0m\n",
      "\u001b[32m2024-09-22 12:30:43.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.36 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:30:43.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 60000: 3.2265625, Perplexity: 25.192907336481866\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:29<00:00,  9.23it/s]Warning: reached the end of the dataset. Training stopped, global_rank=0, update_step=62500\n",
      "\u001b[32m2024-09-22 12:35:17.339\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1081\u001b[0m - \u001b[33m\u001b[1mReached the end of the dataset. Training stopped\u001b[0m\n",
      "\u001b[32m2024-09-22 12:35:17.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1087\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:29<00:00,  8.79it/s]\n",
      "\u001b[32m2024-09-22 12:35:17.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1092\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/expert-feather-740/model_62500, update step 62500\u001b[0m\n",
      "\u001b[32m2024-09-22 12:35:17.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m243\u001b[0m - \u001b[1mSaving took 0.19 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:35:17.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1112\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:14.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 41385760.0 tokens, eval loss: 3.2295\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:14.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 25.27\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:14.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 56.33 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:14.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1131\u001b[0m - \u001b[1mFinal eval loss: 3.2295165061950684\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss ‚ñà‚ñá‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm ‚ñÖ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity ‚ñà‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ‚ñÑ‚ñá‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñÅ‚ñá‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñà‚ñÇ‚ñÜ‚ñÑ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 3.22952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 41385760.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm 1.83594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 3.59375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity 25.26744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 9.20365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 589.03335\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 241503.6732\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 1652867648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 62500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mexpert-feather-740\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/0yhz6j2s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240922_103644-0yhz6j2s/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "\u001b[32m2024-09-22 12:36:22.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1154\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
      "Rank 0 finished successfully\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path bert_c4 \\\n",
    "    --dataset_path /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM \\\n",
    "    --batch_size 64\\\n",
    "    --lr 6e-4 \\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.02 \\\n",
    "    --num_training_steps 62500 \\\n",
    "    --save_every 600000\\\n",
    "    --eval_every 3000 \\\n",
    "    --warmup_steps 9000 \\\n",
    "    --seed 42 \\\n",
    "    --tags relora_bert_medium \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 62500 \\\n",
    "    --restart_warmup_steps 500 \\\n",
    "    --scheduler linear \\\n",
    "    --reset_optimizer_on_relora False \\\n",
    "    --relora 62500\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.9\\\n",
    "    --lora_alpha 64 \\\n",
    "    --min_lr_ratio 0.000000000000000000000000000000000000000000001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81c36797-2beb-4901-84ad-38b1baca6773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-22 12:36:30.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:30.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240922_123630-tw48i0jz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeft-sponge-741\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/tw48i0jz\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_name_or_path             bert_c4\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdataset_path                   /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mbatch_size                     64\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtotal_batch_size               64\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_r                         64\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_alpha                     128.0\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrelora                         62500\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mreset_optimizer_on_relora      False\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlr                             0.0004\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mscheduler                      linear\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcycle_length                   62500\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrestart_warmup_steps           500\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmin_lr_ratio                   1e-45\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mweight_decay                   0.02\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmup_steps                   9000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1meval_every                     3000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mnum_training_steps             62500\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_every                     600000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_dir                       checkpoint/deft-sponge-741\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtags                           ['relora_bert_medium']\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrun_name                       deft-sponge-741\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m473\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:32.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1mUsing HuggingFace model bert_c4 revision None\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m565\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 36685626 || all params: 42977082 || trainable%: 85.36\n",
      "\u001b[32m2024-09-22 12:36:33.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m630\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): BertForMaskedLM(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 512)\n",
      "        (token_type_embeddings): Embedding(1, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-7): 8 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (key): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (value): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mTotal params  before LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m632\u001b[0m - \u001b[1mTotal params  after  LoRA: 42.98M\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mTrainable params: 36.69M\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mIn total, added 1.57M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mSaving model to checkpoint/deft-sponge-741 every 600000 update steps\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m657\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-09-22 12:36:33.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[1mScheduler will run for 62500 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 12:36:33.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1mFull training set size: 4000000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m763\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mTrain set size after shard: 4000000\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 12:36:33.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m772\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-09-22 12:36:33.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m871\u001b[0m - \u001b[1mStarting training at update step 0 with 62500 update steps\u001b[0m\n",
      "Update steps:   0%|                                   | 0/62500 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-22 12:36:49.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m904\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Update steps:   5%|‚ñà                     | 3000/62500 [05:42<1:48:28,  9.14it/s]\u001b[32m2024-09-22 12:42:16.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 3000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:42:30.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10294272.0 tokens, eval loss: 6.8147\u001b[0m\n",
      "\u001b[32m2024-09-22 12:42:30.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 911.11\u001b[0m\n",
      "\u001b[32m2024-09-22 12:42:30.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.92 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:42:30.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 3000: 6.814658164978027, Perplexity: 911.105014943461\u001b[0m\n",
      "Update steps:  10%|‚ñà‚ñà                    | 5999/62500 [11:22<1:44:56,  8.97it/s]\u001b[32m2024-09-22 12:47:56.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 6000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:48:10.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10057856.0 tokens, eval loss: 6.5478\u001b[0m\n",
      "\u001b[32m2024-09-22 12:48:10.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 697.70\u001b[0m\n",
      "\u001b[32m2024-09-22 12:48:10.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.66 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:48:10.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 6000: 6.547784328460693, Perplexity: 697.6965934988822\u001b[0m\n",
      "Update steps:  14%|‚ñà‚ñà‚ñà‚ñè                  | 9000/62500 [17:04<1:40:38,  8.86it/s]\u001b[32m2024-09-22 12:53:37.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 9000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:53:54.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11892160.0 tokens, eval loss: 6.3426\u001b[0m\n",
      "\u001b[32m2024-09-22 12:53:54.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 568.24\u001b[0m\n",
      "\u001b[32m2024-09-22 12:53:54.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 16.15 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:53:54.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 9000: 6.342550754547119, Perplexity: 568.2439151061498\u001b[0m\n",
      "Update steps:  19%|‚ñà‚ñà‚ñà‚ñà                 | 11999/62500 [22:47<1:26:51,  9.69it/s]\u001b[32m2024-09-22 12:59:21.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 12000\u001b[0m\n",
      "\u001b[32m2024-09-22 12:59:32.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8459968.0 tokens, eval loss: 5.6991\u001b[0m\n",
      "\u001b[32m2024-09-22 12:59:32.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 298.60\u001b[0m\n",
      "\u001b[32m2024-09-22 12:59:32.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.46 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 12:59:32.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 12000: 5.699120998382568, Perplexity: 298.60481146341914\u001b[0m\n",
      "Update steps:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà                | 15000/62500 [28:25<1:25:37,  9.25it/s]\u001b[32m2024-09-22 13:04:59.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 15000\u001b[0m\n",
      "\u001b[32m2024-09-22 13:05:10.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8081344.0 tokens, eval loss: 4.7920\u001b[0m\n",
      "\u001b[32m2024-09-22 13:05:10.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 120.54\u001b[0m\n",
      "\u001b[32m2024-09-22 13:05:10.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.98 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 13:05:10.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 15000: 4.791973114013672, Perplexity: 120.53897131106004\u001b[0m\n",
      "Update steps:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 18000/62500 [34:02<1:15:00,  9.89it/s]\u001b[32m2024-09-22 13:10:36.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 18000\u001b[0m\n",
      "\u001b[32m2024-09-22 13:10:49.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9698816.0 tokens, eval loss: 4.3811\u001b[0m\n",
      "\u001b[32m2024-09-22 13:10:49.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 79.92\u001b[0m\n",
      "\u001b[32m2024-09-22 13:10:49.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.13 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 13:10:49.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 18000: 4.381078720092773, Perplexity: 79.92420276383962\u001b[0m\n",
      "Update steps:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 21000/62500 [39:42<1:12:56,  9.48it/s]\u001b[32m2024-09-22 13:16:16.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 21000\u001b[0m\n",
      "\u001b[32m2024-09-22 13:16:31.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10771584.0 tokens, eval loss: 4.1585\u001b[0m\n",
      "\u001b[32m2024-09-22 13:16:31.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 63.97\u001b[0m\n",
      "\u001b[32m2024-09-22 13:16:31.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.63 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 13:16:31.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 21000: 4.158482074737549, Perplexity: 63.97434059334963\u001b[0m\n",
      "Update steps:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 23999/62500 [45:25<1:05:44,  9.76it/s]\u001b[32m2024-09-22 13:21:59.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 24000\u001b[0m\n",
      "\u001b[32m2024-09-22 13:22:12.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9764544.0 tokens, eval loss: 4.0263\u001b[0m\n",
      "\u001b[32m2024-09-22 13:22:12.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 56.06\u001b[0m\n",
      "\u001b[32m2024-09-22 13:22:12.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.22 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 13:22:12.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 24000: 4.026335716247559, Perplexity: 56.055132550164124\u001b[0m\n",
      "Update steps:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 27000/62500 [51:07<1:02:18,  9.49it/s]\u001b[32m2024-09-22 13:27:41.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 27000\u001b[0m\n",
      "\u001b[32m2024-09-22 13:27:55.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10676672.0 tokens, eval loss: 3.9495\u001b[0m\n",
      "\u001b[32m2024-09-22 13:27:55.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 51.91\u001b[0m\n",
      "\u001b[32m2024-09-22 13:27:55.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.50 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 13:27:55.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 27000: 3.949453115463257, Perplexity: 51.90697195086857\u001b[0m\n",
      "Update steps:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 30000/62500 [56:48<58:44,  9.22it/s]\u001b[32m2024-09-22 13:33:22.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 30000\u001b[0m\n",
      "\u001b[32m2024-09-22 13:33:33.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8097664.0 tokens, eval loss: 3.8770\u001b[0m\n",
      "\u001b[32m2024-09-22 13:33:33.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 48.28\u001b[0m\n",
      "\u001b[32m2024-09-22 13:33:33.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 10.98 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 13:33:33.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 30000: 3.8769915103912354, Perplexity: 48.278750247912264\u001b[0m\n",
      "Update steps:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 32999/62500 [1:02:27<55:57,  8.79it/s]\u001b[32m2024-09-22 13:39:01.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 33000\u001b[0m\n",
      "\u001b[32m2024-09-22 13:39:16.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11561856.0 tokens, eval loss: 3.8351\u001b[0m\n",
      "\u001b[32m2024-09-22 13:39:16.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 46.30\u001b[0m\n",
      "\u001b[32m2024-09-22 13:39:16.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.66 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 13:39:16.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 33000: 3.835113286972046, Perplexity: 46.29867240727127\u001b[0m\n",
      "Update steps:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 36000/62500 [1:08:11<47:23,  9.32it/s]\u001b[32m2024-09-22 13:44:45.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 36000\u001b[0m\n",
      "\u001b[32m2024-09-22 13:44:58.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9914560.0 tokens, eval loss: 3.8076\u001b[0m\n",
      "\u001b[32m2024-09-22 13:44:58.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 45.04\u001b[0m\n",
      "\u001b[32m2024-09-22 13:44:58.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.45 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 13:44:58.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 36000: 3.8075568675994873, Perplexity: 45.040265005692696\u001b[0m\n",
      "Update steps:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 39000/62500 [1:13:51<42:27,  9.22it/s]\u001b[32m2024-09-22 13:50:25.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 39000\u001b[0m\n",
      "\u001b[32m2024-09-22 13:50:41.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11786304.0 tokens, eval loss: 3.7732\u001b[0m\n",
      "\u001b[32m2024-09-22 13:50:41.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 43.52\u001b[0m\n",
      "\u001b[32m2024-09-22 13:50:41.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.97 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 13:50:41.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 39000: 3.773193359375, Perplexity: 43.518814393274575\u001b[0m\n",
      "Update steps:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 42000/62500 [1:19:34<36:32,  9.35it/s]\u001b[32m2024-09-22 13:56:08.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 42000\u001b[0m\n",
      "\u001b[32m2024-09-22 13:56:22.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10233600.0 tokens, eval loss: 3.7492\u001b[0m\n",
      "\u001b[32m2024-09-22 13:56:22.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 42.49\u001b[0m\n",
      "\u001b[32m2024-09-22 13:56:22.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.86 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 13:56:22.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 42000: 3.7491841316223145, Perplexity: 42.486404541918276\u001b[0m\n",
      "Update steps:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 45000/62500 [1:25:16<32:05,  9.09it/s]\u001b[32m2024-09-22 14:01:50.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 45000\u001b[0m\n",
      "\u001b[32m2024-09-22 14:02:04.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10827456.0 tokens, eval loss: 3.7407\u001b[0m\n",
      "\u001b[32m2024-09-22 14:02:04.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 42.13\u001b[0m\n",
      "\u001b[32m2024-09-22 14:02:04.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.66 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 14:02:04.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 45000: 3.7407238483428955, Perplexity: 42.12847375683478\u001b[0m\n",
      "Update steps:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 48000/62500 [1:30:58<26:21,  9.17it/s]\u001b[32m2024-09-22 14:07:32.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 48000\u001b[0m\n",
      "\u001b[32m2024-09-22 14:07:44.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8874624.0 tokens, eval loss: 3.7327\u001b[0m\n",
      "\u001b[32m2024-09-22 14:07:44.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 41.79\u001b[0m\n",
      "\u001b[32m2024-09-22 14:07:44.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.98 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 14:07:44.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 48000: 3.7327425479888916, Perplexity: 41.793572007850045\u001b[0m\n",
      "Update steps:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 51000/62500 [1:36:37<18:54, 10.13it/s]\u001b[32m2024-09-22 14:13:11.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 51000\u001b[0m\n",
      "\u001b[32m2024-09-22 14:13:22.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 8136320.0 tokens, eval loss: 3.7238\u001b[0m\n",
      "\u001b[32m2024-09-22 14:13:22.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 41.42\u001b[0m\n",
      "\u001b[32m2024-09-22 14:13:22.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 11.00 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 14:13:22.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 51000: 3.7237541675567627, Perplexity: 41.41959870797377\u001b[0m\n",
      "Update steps:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 54000/62500 [1:42:16<15:12,  9.32it/s]\u001b[32m2024-09-22 14:18:50.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 54000\u001b[0m\n",
      "\u001b[32m2024-09-22 14:19:05.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 11175808.0 tokens, eval loss: 3.7272\u001b[0m\n",
      "\u001b[32m2024-09-22 14:19:05.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 41.56\u001b[0m\n",
      "\u001b[32m2024-09-22 14:19:05.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 15.18 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 14:19:05.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 54000: 3.727174997329712, Perplexity: 41.561530728650396\u001b[0m\n",
      "Update steps:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 57000/62500 [1:48:00<09:51,  9.30it/s]\u001b[32m2024-09-22 14:24:33.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 57000\u001b[0m\n",
      "\u001b[32m2024-09-22 14:24:47.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 9911104.0 tokens, eval loss: 3.7128\u001b[0m\n",
      "\u001b[32m2024-09-22 14:24:47.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 40.97\u001b[0m\n",
      "\u001b[32m2024-09-22 14:24:47.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 13.40 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 14:24:47.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 57000: 3.7127597332000732, Perplexity: 40.96670785032144\u001b[0m\n",
      "Update steps:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 60000/62500 [1:53:41<04:19,  9.64it/s]\u001b[32m2024-09-22 14:30:14.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m989\u001b[0m - \u001b[1mPerforming evaluation at step 60000\u001b[0m\n",
      "\u001b[32m2024-09-22 14:30:29.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 10596096.0 tokens, eval loss: 3.7150\u001b[0m\n",
      "\u001b[32m2024-09-22 14:30:29.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 41.06\u001b[0m\n",
      "\u001b[32m2024-09-22 14:30:29.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 14.36 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 14:30:29.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1000\u001b[0m - \u001b[1mEval loss at step 60000: 3.7149598598480225, Perplexity: 41.056939019544195\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:28<00:00,  9.26it/s]Warning: reached the end of the dataset. Training stopped, global_rank=0, update_step=62500\n",
      "\u001b[32m2024-09-22 14:35:02.391\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1081\u001b[0m - \u001b[33m\u001b[1mReached the end of the dataset. Training stopped\u001b[0m\n",
      "\u001b[32m2024-09-22 14:35:02.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1087\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
      "Update steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62500/62500 [1:58:28<00:00,  8.79it/s]\n",
      "\u001b[32m2024-09-22 14:35:02.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1092\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/deft-sponge-741/model_62500, update step 62500\u001b[0m\n",
      "\u001b[32m2024-09-22 14:35:02.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m243\u001b[0m - \u001b[1mSaving took 0.19 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 14:35:02.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1112\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
      "\u001b[32m2024-09-22 14:35:58.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mEvaluated on 41385760.0 tokens, eval loss: 3.7157\u001b[0m\n",
      "\u001b[32m2024-09-22 14:35:58.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mPerplexity: 41.09\u001b[0m\n",
      "\u001b[32m2024-09-22 14:35:58.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluation took 56.12 seconds\u001b[0m\n",
      "\u001b[32m2024-09-22 14:35:58.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1131\u001b[0m - \u001b[1mFinal eval loss: 3.715651035308838\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss ‚ñà‚ñá‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm ‚ñá‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ‚ñÑ‚ñá‚ñà‚ñÇ‚ñà‚ñÇ‚ñá‚ñÇ‚ñá‚ñá‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñà‚ñÇ‚ñÖ‚ñÉ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñÉ‚ñÉ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 3.71565\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 41385760.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm 2.07812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 4.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perplexity 41.08533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 9.23837\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 591.2558\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 242414.87935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 1652867648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 62500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdeft-sponge-741\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/tw48i0jz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240922_123630-tw48i0jz/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "\u001b[32m2024-09-22 14:36:04.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1154\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
      "Rank 0 finished successfully\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path bert_c4 \\\n",
    "    --dataset_path /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM \\\n",
    "    --batch_size 64\\\n",
    "    --lr 4e-4 \\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.02 \\\n",
    "    --num_training_steps 62500 \\\n",
    "    --save_every 600000\\\n",
    "    --eval_every 3000 \\\n",
    "    --warmup_steps 9000 \\\n",
    "    --seed 42 \\\n",
    "    --tags relora_bert_medium \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 62500 \\\n",
    "    --restart_warmup_steps 500 \\\n",
    "    --scheduler linear \\\n",
    "    --reset_optimizer_on_relora False \\\n",
    "    --relora 62500\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.9\\\n",
    "    --lora_alpha 128 \\\n",
    "    --min_lr_ratio 0.000000000000000000000000000000000000000000001 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7053e94b-18f3-4e0d-bd0c-011f0a00cdb9",
   "metadata": {},
   "source": [
    "compare with standart lora to see if everything is working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fdb885-282f-41c1-b605-152af8a42208",
   "metadata": {},
   "source": [
    "after that test fewer warm_up steps\n",
    "different magnitude pruning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
