{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b6f54c-c279-49bd-b102-e9cec36d5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d01b703-f775-4bb7-9cae-da915253eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric,concatenate_datasets\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments,AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "985163d1-8956-4904-bd60-8fce701b3807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/tmp/ipykernel_108834/1636946568.py:12: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"cola\")\n",
      "/media/.venv/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8551/8551 [00:34<00:00, 245.91it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1043/1043 [00:00<00:00, 1819.99it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1063/1063 [00:00<00:00, 1769.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "from transformers import T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load dataset (example using CoLA)\n",
    "dataset = load_dataset(\"glue\", \"cola\")\n",
    "\n",
    "# Tokenizer and metrics\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "metric = load_metric(\"glue\", \"cola\")\n",
    "\n",
    "# Prepare dataset\n",
    "def preprocess_function(examples):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "    for i in tqdm(range(len(examples))):\n",
    "        tokenized = tokenizer(examples[\"sentence\"][i], truncation=True, max_length=512, padding=\"max_length\")\n",
    "        input_ids.append(tokenized[\"input_ids\"])\n",
    "        attention_mask.append(tokenized[\"attention_mask\"])\n",
    "        labels.append(examples[\"label\"][i])\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": labels}\n",
    "\n",
    "# Convert to Dataset object and preprocess\n",
    "train_dataset = Dataset.from_dict(preprocess_function(dataset['train']))\n",
    "eval_dataset = Dataset.from_dict(preprocess_function(dataset[\"validation\"]))\n",
    "test_dataset = Dataset.from_dict(preprocess_function(dataset[\"test\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21c85b9-7b0e-497e-a688-c7f3ed6e1632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8551"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493c92c3-6b7f-48b0-b90d-bcc04ea63fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 1165.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows 128265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "concatenated_data = concatenate_datasets([train_dataset, train_dataset])\n",
    "for i in tqdm(range(1,num_epochs-1)):\n",
    "    concatenated_data = concatenate_datasets([concatenated_data, train_dataset])\n",
    "train_dataset = concatenated_data.shuffle(seed=42)  \n",
    "print(f\"Number of rows {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e1f45b9-0771-47de-912d-a7369ab7d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_batch_size = 8\n",
    "num_rows = len(train_dataset)\n",
    "\n",
    "# Round down to the nearest hundred\n",
    "rounded_num_rows = (num_rows // set_batch_size) * set_batch_size\n",
    "\n",
    "# Truncate the dataset\n",
    "train_dataset = train_dataset.select(range(rounded_num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c3567f-e1cb-482a-a54e-ed311b064ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3920a137444a039662a0c83792c34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/128264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d1e0e2b8a04346866c6b122588931e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c24597bcd64de0b34e8bbd713dc5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and preprocessing arguments saved to ./preprocessed_cola_dataset\n"
     ]
    }
   ],
   "source": [
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": eval_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Save the dataset to disk\n",
    "save_path = \"./preprocessed_cola_dataset\"\n",
    "dataset_dict.save_to_disk(save_path)\n",
    "\n",
    "# Save preprocessing args to a json file\n",
    "preprocessing_args = {\n",
    "    \"sequence_length\": 512,\n",
    "    \"tokenizer\": \"t5-base\"\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_path, \"args.json\"), \"w\") as f:\n",
    "    json.dump(preprocessing_args, f)\n",
    "\n",
    "print(f\"Dataset and preprocessing arguments saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad05478-f5f9-4ea8-9956-36418c1f288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "964f2b99-3b83-4703-8b76-1878b0808c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows 128264\n"
     ]
    }
   ],
   "source": [
    "#keep origin weight = false !!!!!!!!!!!\n",
    "print(f\"Number of rows {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21344a3c-5cae-462f-a7e3-757015d7fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-29 21:02:04.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:04.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReLoRa/wandb/run-20240729_210205-l1fw4q4h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdashing-river-78\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/l1fw4q4h\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m430\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m431\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m432\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mmodel_name_or_path             t5-base\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mdataset_path                   ./preprocessed_cola_dataset\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mbatch_size                     16\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mtotal_batch_size               16\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mlora_r                         128\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mlora_alpha                     32\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mrelora                         200\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mreset_optimizer_on_relora      True\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.0\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mlr                             0.0004\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mscheduler                      cosine_restarts\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mcycle_length                   200\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mrestart_warmup_steps           100\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mmin_lr_ratio                   0.1\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mweight_decay                   0.0\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mwarmup_steps                   50\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1meval_every                     100\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mnum_training_steps             128000\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1msave_every                     5000\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1msave_dir                       checkpoints/dashing-river-78\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mtags                           ['relora_t5_base']\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mrun_name                       dashing-river-78\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m435\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m438\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m440\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m452\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m458\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:06.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m462\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:07.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m468\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:07.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m500\u001b[0m - \u001b[1mUsing HuggingFace model t5-base revision None\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:08.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m543\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 76923138 || all params: 275695362 || trainable%: 27.90\n",
      "\u001b[32m2024-07-29 21:02:08.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m608\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): T5ForSequenceClassification(\n",
      "    (transformer): T5Model(\n",
      "      (shared): Embedding(32128, 768)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 768)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (relative_attention_bias): Embedding(32, 12)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (wo): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=3072, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-11): 11 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (wo): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=3072, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 768)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (relative_attention_bias): Embedding(32, 12)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (wo): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=3072, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-11): 11 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (wo): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=3072, out_features=128, bias=False)\n",
      "                    (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (classification_head): T5ClassificationHead(\n",
      "      (dense): ReLoRaLinear(\n",
      "        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "        (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): ReLoRaLinear(\n",
      "        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
      "        (lora_B): Linear(in_features=128, out_features=2, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:08.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m609\u001b[0m - \u001b[1mTotal params  before LoRA: 223.50M\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:08.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m610\u001b[0m - \u001b[1mTotal params  after  LoRA: 275.70M\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:08.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m611\u001b[0m - \u001b[1mTrainable params: 76.92M\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:08.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m612\u001b[0m - \u001b[1mIn total, added 52.20M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:08.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m614\u001b[0m - \u001b[1mSaving model to checkpoints/dashing-river-78 every 5000 update steps\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:09.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m635\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-07-29 21:02:09.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m683\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:09.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m699\u001b[0m - \u001b[1mScheduler will run for 128000 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 128264\n",
      "})\n",
      "\u001b[32m2024-07-29 21:02:09.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m740\u001b[0m - \u001b[1mFull training set size: 128264\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:09.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m741\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 128264\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:09.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m744\u001b[0m - \u001b[1mTrain set size after shard: 128264\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 128264\n",
      "})\n",
      "\u001b[32m2024-07-29 21:02:09.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m750\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-07-29 21:02:09.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m786\u001b[0m - \u001b[1mStarting training at update step 0 with 128000 update steps\u001b[0m\n",
      "Update steps:   0%|                                  | 0/128000 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 128264\n",
      "})\n",
      "\u001b[32m2024-07-29 21:02:09.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m817\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delyanh/Projects/ReLoRa/torchrun_main.py\", line 1068, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delyanh/Projects/ReLoRa/torchrun_main.py\", line 830, in main\n",
      "    loss =  model(input_ids=batch['input_ids'], labels=batch['labels']).loss\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1519, in forward\n",
      "    else self._run_ddp_forward(*inputs, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1355, in _run_ddp_forward\n",
      "    return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py\", line 2051, in forward\n",
      "    outputs = self.transformer(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py\", line 1514, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py\", line 1106, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py\", line 686, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      "                             ^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py\", line 593, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      "                       ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py\", line 553, in forward\n",
      "    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/functional.py\", line 1856, in softmax\n",
      "    ret = input.softmax(dim)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 120.69 MiB is free. Process 97867 has 830.00 MiB memory in use. Process 101055 has 1.18 GiB memory in use. Including non-PyTorch memory, this process has 21.53 GiB memory in use. Of the allocated memory 20.92 GiB is allocated by PyTorch, and 13.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mdashing-river-78\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/l1fw4q4h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240729_210205-l1fw4q4h/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "[2024-07-29 21:02:17,168] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 108486) of binary: /media/.venv/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/.venv/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 806, in main\n",
      "    run(args)\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "torchrun_main.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-07-29_21:02:17\n",
      "  host      : radicho\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 108486)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path t5-base \\\n",
    "    --dataset_path ./preprocessed_cola_dataset \\\n",
    "    --batch_size 16 \\\n",
    "    --lr 3e-4 \\\n",
    "    --max_length 512 \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 32500 \\\n",
    "    --restart_warmup_steps 200 \\\n",
    "    --scheduler cosine_restarts \\\n",
    "    --warmup_steps 200 \\\n",
    "    --reset_optimizer_on_relora True \\\n",
    "    --num_training_steps 130000 \\\n",
    "    --save_every 130000 \\\n",
    "    --eval_every 5200 \\\n",
    "    --tags relora_t5_base \\\n",
    "    --use_double_quant False \\\n",
    "    --workers 4 \\\n",
    "    --seed 42\\\n",
    "    --relora 32500\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.1\\\n",
    "    \n",
    "\n",
    "          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4edca4a-81c4-4339-9451-802616a423b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b944d-e50e-42a3-b325-665baa5a688f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
