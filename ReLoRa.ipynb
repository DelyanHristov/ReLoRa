{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b6f54c-c279-49bd-b102-e9cec36d5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d01b703-f775-4bb7-9cae-da915253eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/.venv/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/media/.venv/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric,concatenate_datasets\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments,AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "985163d1-8956-4904-bd60-8fce701b3807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/tmp/ipykernel_2642779/3068275542.py:12: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"cola\")\n",
      "/media/.venv/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 8551/8551 [00:35<00:00, 238.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1043/1043 [00:00<00:00, 1838.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1063/1063 [00:00<00:00, 1795.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "from transformers import T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load dataset (example using CoLA)\n",
    "dataset = load_dataset(\"glue\", \"cola\")\n",
    "\n",
    "# Tokenizer and metrics\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "metric = load_metric(\"glue\", \"cola\")\n",
    "\n",
    "# Prepare dataset\n",
    "def preprocess_function(examples):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "    for i in tqdm(range(len(examples))):\n",
    "        tokenized = tokenizer(examples[\"sentence\"][i], truncation=True, max_length = 512)\n",
    "        input_ids.append(tokenized[\"input_ids\"])\n",
    "        attention_mask.append(tokenized[\"attention_mask\"])\n",
    "        labels.append(examples[\"label\"][i])\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": labels}\n",
    "\n",
    "# Convert to Dataset object and preprocess\n",
    "train_dataset = Dataset.from_dict(preprocess_function(dataset['train']))\n",
    "eval_dataset = Dataset.from_dict(preprocess_function(dataset[\"validation\"]))\n",
    "test_dataset = Dataset.from_dict(preprocess_function(dataset[\"test\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21c85b9-7b0e-497e-a688-c7f3ed6e1632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8551"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493c92c3-6b7f-48b0-b90d-bcc04ea63fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 38/38 [00:00<00:00, 1159.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows 342040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "concatenated_data = concatenate_datasets([train_dataset, train_dataset])\n",
    "for i in tqdm(range(1,num_epochs-1)):\n",
    "    concatenated_data = concatenate_datasets([concatenated_data, train_dataset])\n",
    "train_dataset = concatenated_data.shuffle(seed=42)  \n",
    "print(f\"Number of rows {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e1f45b9-0771-47de-912d-a7369ab7d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_batch_size = 128\n",
    "num_rows = len(train_dataset)\n",
    "\n",
    "# Round down to the nearest hundred\n",
    "rounded_num_rows = (num_rows // set_batch_size) * set_batch_size\n",
    "\n",
    "# Truncate the dataset\n",
    "train_dataset = train_dataset.select(range(rounded_num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c3567f-e1cb-482a-a54e-ed311b064ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0a5cd537ce4f89a99f66b464dbef28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/342016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564e14f4c05345f893e01d56bebc4816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43bfdafcdf841009285a72916a0f788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and preprocessing arguments saved to ./preprocessed_cola_dataset\n"
     ]
    }
   ],
   "source": [
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": eval_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Save the dataset to disk\n",
    "save_path = \"./preprocessed_cola_dataset\"\n",
    "dataset_dict.save_to_disk(save_path)\n",
    "\n",
    "# Save preprocessing args to a json file\n",
    "preprocessing_args = {\n",
    "    \"sequence_length\": 512,\n",
    "    \"tokenizer\": \"t5-base\"\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_path, \"args.json\"), \"w\") as f:\n",
    "    json.dump(preprocessing_args, f)\n",
    "\n",
    "print(f\"Dataset and preprocessing arguments saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad05478-f5f9-4ea8-9956-36418c1f288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20f4317b-3edd-444f-a99d-b6448f242ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows 342016\n",
      "Number of steps 2672.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows {len(train_dataset)}\")\n",
    "print(f\"Number of steps {len(train_dataset)/set_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce6c8e4d-2b44-4602-a3bb-79154db7cc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delyanh/Projects/ReLoRa/torchrun_main.py:153: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"cola\")\n",
      "/media/.venv/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-07-31 21:35:32.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m379\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:32.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m383\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReLoRa/wandb/run-20240731_213533-zctrlx2z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvaliant-breeze-149\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/zctrlx2z\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m455\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mmodel_name_or_path             t5-base\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mdataset_path                   ./preprocessed_cola_dataset\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mbatch_size                     128\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mtotal_batch_size               128\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mlora_r                         64\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mlora_alpha                     32\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mrelora                         340\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mreset_optimizer_on_relora      False\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.9\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mlr                             0.0003\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mscheduler                      cosine_restarts\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mcycle_length                   340\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mrestart_warmup_steps           10\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mmin_lr_ratio                   0.1\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mweight_decay                   0.01\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mwarmup_steps                   40\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1meval_every                     320\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mnum_training_steps             2720\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1msave_every                     640\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1msave_dir                       checkpoint/valiant-breeze-149\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mtags                           ['relora_t5_base']\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mrun_name                       valiant-breeze-149\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m458\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m461\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m463\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m483\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:35.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m487\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:36.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m493\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:36.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1mUsing HuggingFace model t5-base revision None\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m568\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 50823298 || all params: 249595522 || trainable%: 20.36\n",
      "\u001b[32m2024-07-31 21:35:37.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): T5ForSequenceClassification(\n",
      "    (transformer): T5Model(\n",
      "      (shared): Embedding(32128, 768)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 768)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (relative_attention_bias): Embedding(32, 12)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (wo): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-11): 11 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (wo): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 768)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (relative_attention_bias): Embedding(32, 12)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (wo): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-11): 11 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (k): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (v): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (o): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (wo): ReLoRaLinear(\n",
      "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (lora_A): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (classification_head): T5ClassificationHead(\n",
      "      (dense): ReLoRaLinear(\n",
      "        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "        (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (out_proj): ReLoRaLinear(\n",
      "        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
      "        (lora_B): Linear(in_features=64, out_features=2, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mTotal params  before LoRA: 223.50M\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m635\u001b[0m - \u001b[1mTotal params  after  LoRA: 249.60M\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mTrainable params: 50.82M\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1mIn total, added 26.10M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m639\u001b[0m - \u001b[1mSaving model to checkpoint/valiant-breeze-149 every 640 update steps\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m660\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-07-31 21:35:37.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m708\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m724\u001b[0m - \u001b[1mScheduler will run for 2720 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 342016\n",
      "})\n",
      "\u001b[32m2024-07-31 21:35:37.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m765\u001b[0m - \u001b[1mFull training set size: 342016\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 342016\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m769\u001b[0m - \u001b[1mTrain set size after shard: 342016\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 342016\n",
      "})\n",
      "\u001b[32m2024-07-31 21:35:37.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m775\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-07-31 21:35:37.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m811\u001b[0m - \u001b[1mStarting training at update step 0 with 2720 update steps\u001b[0m\n",
      "Update steps:   0%|                                    | 0/2720 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 342016\n",
      "})\n",
      "\u001b[32m2024-07-31 21:35:38.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m844\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Update steps:  12%|███                       | 320/2720 [00:27<03:17, 12.18it/s]\u001b[32m2024-07-31 21:36:04.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m928\u001b[0m - \u001b[1mPerforming evaluation at step 320\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:04.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluated on 34514.0 tokens, eval loss: 0.6089\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:04.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mEvaluation F1 Score: 0.5636\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:05.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mEvaluation metric Score: -0.0399\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:05.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mEvaluation took 0.28 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:05.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m940\u001b[0m - \u001b[1mEval loss at step 320: 0.6089409589767456, F1 Score: 0.5635510987282111, Metric Score: -0.03990863020638864\u001b[0m\n",
      "Update steps:  12%|███▎                      | 340/2720 [00:29<03:21, 11.83it/s]\u001b[32m2024-07-31 21:36:06.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m955\u001b[0m - \u001b[1margs.resume_from=None, local_step=341, args.relora=340, thresh: 341\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:06.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m956\u001b[0m - \u001b[1mPerforming lora reset at update step 341. Current lr is 2.9118397461536355e-05\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:06.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m967\u001b[0m - \u001b[1mLoRA reset took 0.01s\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:06.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m976\u001b[0m - \u001b[1mPerforming optimizer reset at update step 341. Current lr is 2.9118397461536355e-05\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:06.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:06.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 100.00\u001b[0m\n",
      "Update steps:  13%|███▎                      | 342/2720 [00:29<04:12,  9.40it/s]\u001b[32m2024-07-31 21:36:06.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m990\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 5.823679492307271e-05\u001b[0m\n",
      "Update steps:  24%|██████                    | 640/2720 [00:54<03:01, 11.46it/s]\u001b[32m2024-07-31 21:36:31.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m903\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/valiant-breeze-149/model_640, update step 640\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:32.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mSaving took 0.63 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:32.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m928\u001b[0m - \u001b[1mPerforming evaluation at step 640\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:32.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluated on 36102.0 tokens, eval loss: 0.5280\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:32.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mEvaluation F1 Score: 0.7960\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:32.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mEvaluation metric Score: 0.5406\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:32.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mEvaluation took 0.27 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:32.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m940\u001b[0m - \u001b[1mEval loss at step 640: 0.5279948115348816, F1 Score: 0.7960282455021619, Metric Score: 0.5405955943697894\u001b[0m\n",
      "Update steps:  25%|██████▌                   | 680/2720 [00:58<02:56, 11.53it/s]\u001b[32m2024-07-31 21:36:35.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m955\u001b[0m - \u001b[1margs.resume_from=None, local_step=681, args.relora=340, thresh: 681\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:35.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m956\u001b[0m - \u001b[1mPerforming lora reset at update step 681. Current lr is 2.6267099616445675e-05\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:35.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m967\u001b[0m - \u001b[1mLoRA reset took 0.01s\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:35.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m976\u001b[0m - \u001b[1mPerforming optimizer reset at update step 681. Current lr is 2.6267099616445675e-05\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:35.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:35.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.05\u001b[0m\n",
      "Update steps:  25%|██████▌                   | 682/2720 [00:58<03:27,  9.82it/s]\u001b[32m2024-07-31 21:36:35.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m990\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 5.253419923289135e-05\u001b[0m\n",
      "Update steps:  35%|█████████▏                | 960/2720 [01:21<02:28, 11.89it/s]\u001b[32m2024-07-31 21:36:58.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m928\u001b[0m - \u001b[1mPerforming evaluation at step 960\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:58.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluated on 35386.0 tokens, eval loss: 0.5790\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:58.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mEvaluation F1 Score: 0.8108\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:58.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mEvaluation metric Score: 0.5600\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:58.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mEvaluation took 0.27 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:36:58.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m940\u001b[0m - \u001b[1mEval loss at step 960: 0.5789930820465088, F1 Score: 0.8107784634371713, Metric Score: 0.5599999927061219\u001b[0m\n",
      "Update steps:  38%|█████████▍               | 1020/2720 [01:26<02:13, 12.71it/s]\u001b[32m2024-07-31 21:37:03.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m955\u001b[0m - \u001b[1margs.resume_from=None, local_step=1021, args.relora=340, thresh: 1021\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:03.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m956\u001b[0m - \u001b[1mPerforming lora reset at update step 1021. Current lr is 2.1884724901164065e-05\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:03.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m967\u001b[0m - \u001b[1mLoRA reset took 0.01s\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:03.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m976\u001b[0m - \u001b[1mPerforming optimizer reset at update step 1021. Current lr is 2.1884724901164065e-05\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:03.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:04.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.04\u001b[0m\n",
      "Update steps:  38%|█████████▍               | 1022/2720 [01:26<02:43, 10.39it/s]\u001b[32m2024-07-31 21:37:04.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m990\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 4.376944980232813e-05\u001b[0m\n",
      "Update steps:  47%|███████████▊             | 1280/2720 [01:48<02:07, 11.25it/s]\u001b[32m2024-07-31 21:37:25.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m903\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/valiant-breeze-149/model_1280, update step 1280\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:25.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mSaving took 0.61 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:25.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m928\u001b[0m - \u001b[1mPerforming evaluation at step 1280\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:26.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluated on 35917.0 tokens, eval loss: 0.7977\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:26.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mEvaluation F1 Score: 0.8114\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:26.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mEvaluation metric Score: 0.5650\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:26.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mEvaluation took 0.28 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:26.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m940\u001b[0m - \u001b[1mEval loss at step 1280: 0.7977430820465088, F1 Score: 0.8113731635455409, Metric Score: 0.5650034395953757\u001b[0m\n",
      "Update steps:  50%|████████████▌            | 1360/2720 [01:55<01:51, 12.18it/s]\u001b[32m2024-07-31 21:37:32.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m955\u001b[0m - \u001b[1margs.resume_from=None, local_step=1361, args.relora=340, thresh: 1361\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:32.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m956\u001b[0m - \u001b[1mPerforming lora reset at update step 1361. Current lr is 1.6658248244438998e-05\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:32.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m967\u001b[0m - \u001b[1mLoRA reset took 0.01s\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:32.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m976\u001b[0m - \u001b[1mPerforming optimizer reset at update step 1361. Current lr is 1.6658248244438998e-05\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:32.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:33.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.04\u001b[0m\n",
      "Update steps:  50%|████████████▌            | 1362/2720 [01:55<02:14, 10.07it/s]\u001b[32m2024-07-31 21:37:33.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m990\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 3.3316496488877996e-05\u001b[0m\n",
      "Update steps:  59%|██████████████▋          | 1600/2720 [02:15<01:33, 12.04it/s]\u001b[32m2024-07-31 21:37:52.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m928\u001b[0m - \u001b[1mPerforming evaluation at step 1600\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:53.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluated on 35599.0 tokens, eval loss: 0.7528\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:53.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mEvaluation F1 Score: 0.8151\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:53.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mEvaluation metric Score: 0.5702\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:53.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mEvaluation took 0.28 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:37:53.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m940\u001b[0m - \u001b[1mEval loss at step 1600: 0.7528212070465088, F1 Score: 0.8151231884391915, Metric Score: 0.5702308058201745\u001b[0m\n",
      "Update steps:  62%|███████████████▋         | 1700/2720 [02:24<01:26, 11.83it/s]\u001b[32m2024-07-31 21:38:01.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m955\u001b[0m - \u001b[1margs.resume_from=None, local_step=1701, args.relora=340, thresh: 1701\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:01.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m956\u001b[0m - \u001b[1mPerforming lora reset at update step 1701. Current lr is 1.1406964814329438e-05\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:01.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m967\u001b[0m - \u001b[1mLoRA reset took 0.01s\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:01.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m976\u001b[0m - \u001b[1mPerforming optimizer reset at update step 1701. Current lr is 1.1406964814329438e-05\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:01.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:01.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.04\u001b[0m\n",
      "Update steps:  63%|███████████████▋         | 1702/2720 [02:24<01:40, 10.12it/s]\u001b[32m2024-07-31 21:38:01.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m990\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 2.2813929628658876e-05\u001b[0m\n",
      "Update steps:  71%|█████████████████▋       | 1920/2720 [02:42<01:13, 10.93it/s]\u001b[32m2024-07-31 21:38:19.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m903\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/valiant-breeze-149/model_1920, update step 1920\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:20.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mSaving took 0.59 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:20.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m928\u001b[0m - \u001b[1mPerforming evaluation at step 1920\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:20.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluated on 35400.0 tokens, eval loss: 0.8932\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:20.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mEvaluation F1 Score: 0.8126\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:20.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mEvaluation metric Score: 0.5593\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:20.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mEvaluation took 0.27 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:20.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m940\u001b[0m - \u001b[1mEval loss at step 1920: 0.8932291865348816, F1 Score: 0.8125797780635413, Metric Score: 0.5593209709812511\u001b[0m\n",
      "Update steps:  75%|██████████████████▊      | 2040/2720 [02:53<01:02, 10.91it/s]\u001b[32m2024-07-31 21:38:30.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m955\u001b[0m - \u001b[1margs.resume_from=None, local_step=2041, args.relora=340, thresh: 2041\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:30.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m956\u001b[0m - \u001b[1mPerforming lora reset at update step 2041. Current lr is 6.9540584539816095e-06\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:30.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m967\u001b[0m - \u001b[1mLoRA reset took 0.01s\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:30.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m976\u001b[0m - \u001b[1mPerforming optimizer reset at update step 2041. Current lr is 6.9540584539816095e-06\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:30.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:30.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.03\u001b[0m\n",
      "Update steps:  75%|██████████████████▊      | 2042/2720 [02:53<01:10,  9.62it/s]\u001b[32m2024-07-31 21:38:31.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m990\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 1.3908116907963219e-05\u001b[0m\n",
      "Update steps:  82%|████████████████████▌    | 2240/2720 [03:10<00:38, 12.47it/s]\u001b[32m2024-07-31 21:38:47.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m928\u001b[0m - \u001b[1mPerforming evaluation at step 2240\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:47.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluated on 35936.0 tokens, eval loss: 0.8576\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:47.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mEvaluation F1 Score: 0.8093\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:47.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mEvaluation metric Score: 0.5532\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:47.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mEvaluation took 0.28 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:47.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m940\u001b[0m - \u001b[1mEval loss at step 2240: 0.8576388955116272, F1 Score: 0.8092607441320263, Metric Score: 0.5532122564572604\u001b[0m\n",
      "Update steps:  88%|█████████████████████▉   | 2380/2720 [03:22<00:27, 12.26it/s]\u001b[32m2024-07-31 21:38:59.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m955\u001b[0m - \u001b[1margs.resume_from=None, local_step=2381, args.relora=340, thresh: 2381\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:59.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m956\u001b[0m - \u001b[1mPerforming lora reset at update step 2381. Current lr is 3.997560534138884e-06\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:59.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m967\u001b[0m - \u001b[1mLoRA reset took 0.01s\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:59.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m976\u001b[0m - \u001b[1mPerforming optimizer reset at update step 2381. Current lr is 3.997560534138884e-06\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:59.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mPerforming magnitude pruning of optimizer states. Pruning 0.9 percent\u001b[0m\n",
      "\u001b[32m2024-07-31 21:38:59.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft_pretraining.training_utils\u001b[0m:\u001b[36moptimizer_reset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mPercent of optimizer states zeroed: 90.03\u001b[0m\n",
      "Update steps:  88%|█████████████████████▉   | 2382/2720 [03:22<00:32, 10.26it/s]\u001b[32m2024-07-31 21:38:59.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m990\u001b[0m - \u001b[1mFirst step after optimizer reset lr is 7.995121068277767e-06\u001b[0m\n",
      "Update steps:  94%|███████████████████████▌ | 2560/2720 [03:37<00:12, 12.69it/s]\u001b[32m2024-07-31 21:39:14.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m903\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/valiant-breeze-149/model_2560, update step 2560\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:15.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mSaving took 0.61 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:15.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m928\u001b[0m - \u001b[1mPerforming evaluation at step 2560\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:15.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluated on 35168.0 tokens, eval loss: 0.9692\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:15.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mEvaluation F1 Score: 0.8108\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:15.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mEvaluation metric Score: 0.5600\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:15.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mEvaluation took 0.27 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:15.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m940\u001b[0m - \u001b[1mEval loss at step 2560: 0.9691840410232544, F1 Score: 0.8107784634371713, Metric Score: 0.5599999927061219\u001b[0m\n",
      "Update steps:  98%|████████████████████████▌| 2672/2720 [03:47<00:03, 12.31it/s]Warning: reached the end of the dataset. Training stopped, global_rank=0, update_step=2672\n",
      "\u001b[32m2024-07-31 21:39:24.619\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1021\u001b[0m - \u001b[33m\u001b[1mReached the end of the dataset. Training stopped\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:24.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1027\u001b[0m - \u001b[1mTraining finished\u001b[0m\n",
      "Update steps:  98%|████████████████████████▌| 2672/2720 [03:47<00:04, 11.75it/s]\n",
      "\u001b[32m2024-07-31 21:39:24.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1032\u001b[0m - \u001b[1mSaving model and optimizer to checkpoint/valiant-breeze-149/model_2672, update step 2672\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:25.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model_ddp\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mSaving took 0.59 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:25.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1052\u001b[0m - \u001b[1mRunning final evaluation\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:25.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mEvaluated on 33817.0 tokens, eval loss: 0.9332\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:25.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mEvaluation F1 Score: 0.8111\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:25.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mEvaluation metric Score: 0.5601\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:25.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mEvaluation took 0.26 seconds\u001b[0m\n",
      "\u001b[32m2024-07-31 21:39:25.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mFinal eval loss: 0.9331597089767456\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            f1_score ▁▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss ▂▁▂▅▅▇▆█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens ▃█▆▇▆▆▇▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm ▄▁▂▄▅▁▄▂▂▄▅▅▄▃▄▂▅▄▆▄▇▄▅▄▄▄█▂▃▃▄▄▄▄▄▄▄▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss █▆▇▇▆▆▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▃▁▁▁▁▂▁▁▂▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr ▆███████▇▇▇▇▇▆▆▂▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metric_score ▁████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches █▇▇▆▆▃▇▆█▆▅▇▁▆▇▄▆▇▇▇▅▆▆▆▇▇▅▅▆▅▇█▁▇▃▇▇▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples █▇▇▆▆▃▇▆█▆▅▇▁▆▇▄▆▇▇▇▅▆▆▆▇▇▅▅▆▅▇█▁▇▃▇▇▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ▅▆▆█▆▇▃▅▁▅▇▄▅▅▄▅▅▅▄▅▆▅▅▅▄▄▆▇▅▇▃▁▅▄▇▄▄▅▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            f1_score 0.81106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     final_eval_loss 0.93316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   final_eval_tokens 33817.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm 1.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 0.07666\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metric_score 0.56009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 13.56274\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 1736.03053\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 45136.79366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 10136704\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 2672\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvaliant-breeze-149\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/zctrlx2z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240731_213533-zctrlx2z/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "100%|███████████████████████████████████████| 1043/1043 [00:16<00:00, 61.69it/s]\n",
      "/media/.venv/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "{'matthews_correlation': 0.5416905121171213}\n",
      "\u001b[32m2024-07-31 21:39:51.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1131\u001b[0m - \u001b[1mScript finished successfully\u001b[0m\n",
      "Rank 0 finished successfully\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path t5-base \\\n",
    "    --dataset_path ./preprocessed_cola_dataset \\\n",
    "    --batch_size 128 \\\n",
    "    --lr 3e-4 \\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.01\\\n",
    "    --num_training_steps 2720 \\\n",
    "    --save_every 640\\\n",
    "    --eval_every 320 \\\n",
    "    --warmup_steps 40 \\\n",
    "    --seed 42\\\n",
    "    --tags relora_t5_base \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 340 \\\n",
    "    --restart_warmup_steps 10 \\\n",
    "    --scheduler cosine_restarts \\\n",
    "    --reset_optimizer_on_relora False \\\n",
    "    --relora 340\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af04b20-e292-417f-8b93-3df17998abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft_pretraining.relora import ReLoRaModel \n",
    "model = ReLoRaModel.from_pretrained(\"checkpoint/valiant-breeze-149/model_1920\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83440363-d31d-400b-8b3d-025fa5bb80b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLoRaModel(\n",
       "  (wrapped_model): T5ForSequenceClassification(\n",
       "    (transformer): T5Model(\n",
       "      (shared): Embedding(32128, 768)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (v): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (wo): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (v): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (wo): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (v): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (v): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (wo): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (v): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (v): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (wo): ReLoRaLinear(\n",
       "                    (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_A): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                    (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (classification_head): T5ClassificationHead(\n",
       "      (dense): ReLoRaLinear(\n",
       "        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "        (lora_B): Linear(in_features=64, out_features=768, bias=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (out_proj): ReLoRaLinear(\n",
       "        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lora_A): Linear(in_features=768, out_features=64, bias=False)\n",
       "        (lora_B): Linear(in_features=64, out_features=2, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee74b0f3-e473-44eb-ad58-7ba82b371346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1043/1043 [00:15<00:00, 65.51it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for i in tqdm(range(len(eval_dataset))):\n",
    "   #print(eval_dataset[i])\n",
    "   inputs  = eval_dataset[i]\n",
    "   #print(input)\n",
    "   output = eval_dataset[i][\"label\"]\n",
    "\n",
    "   model.eval()\n",
    "# Forward pass to get logits (predictions)\n",
    "   with torch.no_grad():\n",
    "\n",
    "     outputs = model(input_ids = torch.tensor(inputs[\"input_ids\"]).unsqueeze(0).to(\"cuda\"), attention_mask = torch.tensor(inputs[\"attention_mask\"]).unsqueeze(0).to(\"cuda\"))\n",
    "     logits = outputs.logits\n",
    "\n",
    "# Convert logits to probabilities\n",
    "   probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Get predicted class label\n",
    "   predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "   all_predictions.append(predicted_class)\n",
    "   all_labels.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5cec4d1-a159-4114-b6ef-8fc81e3c47af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matthews_correlation': 0.5554433905906734}\n"
     ]
    }
   ],
   "source": [
    "results = metric.compute(predictions=all_predictions, references=all_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3802f41-f5b7-4d6f-a7b0-a5d19e6b5e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
