# dataset
megatron_dataset_config: configs/pile_megatron_dataset.yaml
max_length: 2048
workers: 8

# model
model_name_or_path: EleutherAI/pythia-1b
model_revision: step10000

# saving
save_dir: checkpoints/relora_1b_Aug4_2023_run
autoresume: true

# ReLoRA
use_peft: true
force_keep_original: true
lora_r: 128
relora: 5320
restart_warmup_steps: 100
reset_optimizer_on_relora: false
optimizer_magnitude_pruning: 0.9

# Optimization
optimizer: adam_zero
batch_size: 64
total_batch_size: 1024
lr: 4e-4
adam_beta1: 0.9
adam_beta2: 0.95
weight_decay: 0.01
scheduler: cosine_restarts
warmup_steps: 13_000
num_training_steps: 133_000
eval_every: 1000
save_every: 1000

# Misc
dtype: bfloat16
distributed_type: ddp
tags: relora1b
