{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b6f54c-c279-49bd-b102-e9cec36d5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d01b703-f775-4bb7-9cae-da915253eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric,concatenate_datasets\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments,AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd8039b-fac5-49c8-bbc6-4f025701517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_requires_grad(model):\n",
    "    print(\"Checking which high-level components require gradients:\\n\")\n",
    "\n",
    "    # Checking Embeddings\n",
    "    print(\"Embeddings:\")\n",
    "    for name, param in model.wrapped_model.bert.embeddings.named_parameters():\n",
    "        grad_status = \"requires gradient\" if param.requires_grad else \"does NOT require gradient\"\n",
    "        print(f\"  {name} - {grad_status}\")\n",
    "\n",
    "    # Checking Encoder\n",
    "    print(\"\\nEncoder:\")\n",
    "    for i, layer in enumerate(model.wrapped_model.bert.encoder.layer):\n",
    "        print(f\"  Layer {i}:\")\n",
    "        for name, param in layer.named_parameters():\n",
    "            grad_status = \"requires gradient\" if param.requires_grad else \"does NOT require gradient\"\n",
    "            print(f\"    {name} - {grad_status}\")\n",
    "\n",
    "    # Checking Attention\n",
    "    print(\"\\nAttention:\")\n",
    "    for i, layer in enumerate(model.wrapped_model.bert.encoder.layer):\n",
    "        print(f\"  Layer {i} Attention:\")\n",
    "        for name, param in layer.attention.named_parameters():\n",
    "            grad_status = \"requires gradient\" if param.requires_grad else \"does NOT require gradient\"\n",
    "            print(f\"    {name} - {grad_status}\")\n",
    "\n",
    "    # Checking MLM Head\n",
    "    print(\"\\nMasked LM Head:\")\n",
    "    for name, param in model.wrapped_model.cls.named_parameters():\n",
    "        grad_status = \"requires gradient\" if param.requires_grad else \"does NOT require gradient\"\n",
    "        print(f\"  {name} - {grad_status}\")\n",
    "\n",
    "    print(\"\\nCheck complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a7b65-cbe7-45ff-b567-36cd2dce2c6c",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a8da252-cfc2-48ec-a250-75bbf2f5d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments,AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import AutoModelForMaskedLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BertConfig, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ccaca1b-f380-4553-a799-abb8b240e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6408483-5080-43b7-b864-843754789697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/.venv/lib/python3.11/site-packages/datasets/load.py:1491: FutureWarning: The repository for c4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/c4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/delyanh/.cache/huggingface/modules/datasets_modules/datasets/c4/584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db/c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-medium\")\n",
    "train_data = load_dataset(\"c4\", \"en\", split='train', streaming=True)\n",
    "test_data = load_dataset(\"c4\", \"en\", split='validation', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21587357-6fbe-40b2-8086-d1d02a6576f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "load_path = \"/home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\"\n",
    "loaded_dataset_dict = load_from_disk(load_path)\n",
    "\n",
    "# Access different splits\n",
    "train_data1 = loaded_dataset_dict[\"train\"]\n",
    "validation_data1 = loaded_dataset_dict[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5c302a-a5ae-4826-9855-d108ab1d43a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/.venv/lib/python3.11/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240921_224919-1xncdxfy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delyanhristov06-/huggingface/runs/1xncdxfy' target=\"_blank\">model_BERT</a></strong> to <a href='https://wandb.ai/delyanhristov06-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delyanhristov06-/huggingface' target=\"_blank\">https://wandb.ai/delyanhristov06-/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delyanhristov06-/huggingface/runs/1xncdxfy' target=\"_blank\">https://wandb.ai/delyanhristov06-/huggingface/runs/1xncdxfy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62500' max='62500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62500/62500 2:48:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.858400</td>\n",
       "      <td>6.752958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.545700</td>\n",
       "      <td>6.449647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.149600</td>\n",
       "      <td>5.025860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.267800</td>\n",
       "      <td>4.156699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.058400</td>\n",
       "      <td>3.831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.789100</td>\n",
       "      <td>3.650591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.611400</td>\n",
       "      <td>3.527576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.676600</td>\n",
       "      <td>3.441055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.472200</td>\n",
       "      <td>3.374135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.452700</td>\n",
       "      <td>3.327477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.308800</td>\n",
       "      <td>3.291660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.346900</td>\n",
       "      <td>3.256433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.322900</td>\n",
       "      <td>3.234123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.390300</td>\n",
       "      <td>3.211485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.275700</td>\n",
       "      <td>3.199928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>3.343500</td>\n",
       "      <td>3.183143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>3.257700</td>\n",
       "      <td>3.176211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>3.246400</td>\n",
       "      <td>3.178562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>3.278300</td>\n",
       "      <td>3.175316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>3.369600</td>\n",
       "      <td>3.169739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 01:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 23.90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = BertConfig(\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=2048,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=1,  # Adjusted from 2 to 1 based on your config\n",
    "    vocab_size=30522,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"model_BERT\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    #save_steps=50,\n",
    "    eval_steps=3000,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.03,\n",
    "    logging_steps=5,\n",
    "    num_train_epochs=1,\n",
    "    save_strategy  = \"no\",\n",
    "    warmup_ratio = 0.15,\n",
    "    bf16 = True,\n",
    "    seed = 42\n",
    "   # max_steps = 4000\n",
    "    #load_best_model_at_end=True,  # Load the best model at the end\n",
    "    #metric_for_best_model=\"eval_f1_macro\",   # Metric to compare for best model\n",
    "    #greater_is_better=True\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_config(config).to(torch.bfloat16)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data1,\n",
    "    eval_dataset=validation_data1,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96cae3af-c0b4-4bc7-8b16-6f7dcda5a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.42.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules = target_modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model12 = get_peft_model(model12, config)\n",
    "print(model12.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6921047d-d199-4569-b706-0386ebc01a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240921_160935-7v5szsc9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delyanhristov06-/bert-lora-experiment/runs/7v5szsc9' target=\"_blank\">cerulean-gorge-48</a></strong> to <a href='https://wandb.ai/delyanhristov06-/bert-lora-experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delyanhristov06-/bert-lora-experiment' target=\"_blank\">https://wandb.ai/delyanhristov06-/bert-lora-experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delyanhristov06-/bert-lora-experiment/runs/7v5szsc9' target=\"_blank\">https://wandb.ai/delyanhristov06-/bert-lora-experiment/runs/7v5szsc9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft_pretraining.relora import ReLoRaModel, ReLoRaLinear, merge_and_reinit_functional\n",
    "#parameters after grid search 1\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import wandb\n",
    "wandb.init(project=\"bert-lora-experiment\")\n",
    "config = BertConfig(\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=2048,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=1,  # Adjusted from 2 to 1 based on your config\n",
    "    vocab_size=30522,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "target_modules = [\"query\", \"key\", \"value\"]\n",
    "\n",
    "\n",
    "model = ReLoRaModel(\n",
    "            model,\n",
    "            r=64,\n",
    "            lora_alpha=128,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            trainable_scaling= False,\n",
    "            keep_original_weights=True,\n",
    "            lora_only= False,\n",
    "            quantize=None,\n",
    "            use_double_quant=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af191f6-0e15-405e-b22e-fe5929ada2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-21 20:18:57.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:57.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240921_201858-3brg1tmr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mradiant-breeze-729\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/3brg1tmr\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_name_or_path             bert_c4\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdataset_path                   /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mbatch_size                     64\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtotal_batch_size               64\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_r                         64\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_alpha                     64.0\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrelora                         15625\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mreset_optimizer_on_relora      False\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.9\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlr                             0.001\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mscheduler                      cosine_restarts\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcycle_length                   15625\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrestart_warmup_steps           500\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmin_lr_ratio                   1e-45\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mweight_decay                   0.02\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmup_steps                   9000\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1meval_every                     3500\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mnum_training_steps             62500\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_every                     600000\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_dir                       checkpoint/radiant-breeze-729\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtags                           ['relora_bert_medium']\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrun_name                       radiant-breeze-729\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m473\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-09-21 20:18:59.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1mUsing HuggingFace model bert_c4 revision None\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m565\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 36685626 || all params: 42977082 || trainable%: 85.36\n",
      "\u001b[32m2024-09-21 20:19:00.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m630\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): BertForMaskedLM(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 512)\n",
      "        (token_type_embeddings): Embedding(1, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-7): 8 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (key): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (value): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mTotal params  before LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m632\u001b[0m - \u001b[1mTotal params  after  LoRA: 42.98M\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mTrainable params: 36.69M\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mIn total, added 1.57M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mSaving model to checkpoint/radiant-breeze-729 every 600000 update steps\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m657\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-09-21 20:19:00.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[1mScheduler will run for 62500 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-21 20:19:00.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1mFull training set size: 4000000\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m763\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mTrain set size after shard: 4000000\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-21 20:19:00.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m772\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "\u001b[32m2024-09-21 20:19:00.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m871\u001b[0m - \u001b[1mStarting training at update step 0 with 62500 update steps\u001b[0m\n",
      "Update steps:   0%|                                   | 0/62500 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-21 20:19:17.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m904\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "Update steps:   0%|                        | 64/62500 [00:23<2:00:32,  8.63it/s]^C\n",
      "W0921 20:19:24.547000 139879205171264 torch/distributed/elastic/agent/server/api.py:741] Received 2 death signal, shutting down workers\n",
      "W0921 20:19:24.547000 139879205171264 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 192308 closing signal SIGINT\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delyanh/Projects/ReVeRA/ReLoRa/torchrun_main.py\", line 1160, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delyanh/Projects/ReVeRA/ReLoRa/torchrun_main.py\", line 929, in main\n",
      "    scaled_loss.backward()\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/_tensor.py\", line 525, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 267, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/delyanh/Projects/ReVeRA/ReLoRa/torchrun_main.py\", line 1160, in <module>\n",
      "[rank0]:     main(args)\n",
      "[rank0]:   File \"/home/delyanh/Projects/ReVeRA/ReLoRa/torchrun_main.py\", line 929, in main\n",
      "[rank0]:     scaled_loss.backward()\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/_tensor.py\", line 525, in backward\n",
      "[rank0]:     torch.autograd.backward(\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 267, in backward\n",
      "[rank0]:     _engine_run_backward(\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n",
      "[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]: KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#chocolate-galaxy-701\n",
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path bert_c4 \\\n",
    "    --dataset_path /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM \\\n",
    "    --batch_size 64\\\n",
    "    --lr 1e-3\\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.02\\\n",
    "    --num_training_steps 62500 \\\n",
    "    --save_every 600000\\\n",
    "    --eval_every 3500 \\\n",
    "    --warmup_steps 9000 \\\n",
    "    --seed 42\\\n",
    "    --tags relora_bert_medium \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 15625 \\\n",
    "    --restart_warmup_steps 500 \\\n",
    "    --scheduler cosine_restarts \\\n",
    "    --reset_optimizer_on_relora False \\\n",
    "    --relora 15625\\\n",
    "    --lora_r 64\\\n",
    "    --optimizer_magnitude_pruning 0.9\\\n",
    "    --lora_alpha 64\\\n",
    "    --min_lr_ratio 0.000000000000000000000000000000000000000000001\\\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad8613-bb9f-4a9c-aace-38cee60e5dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft_pretraining.relora import ReLoRaModel, ReLoRaLinear, merge_and_reinit_functional\n",
    "#parameters after grid search 1\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import wandb\n",
    "wandb.init(project=\"bert-lora-experiment\")\n",
    "config = BertConfig(\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=2048,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=1,  # Adjusted from 2 to 1 based on your config\n",
    "    vocab_size=30522,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "target_modules = [\"query\", \"key\", \"value\"]\n",
    "\n",
    "\n",
    "model = ReLoRaModel(\n",
    "            model,\n",
    "            r=64,\n",
    "            lora_alpha=128,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            trainable_scaling= False,\n",
    "            keep_original_weights=True,\n",
    "            lora_only= False,\n",
    "            quantize=None,\n",
    "            use_double_quant=False,\n",
    "        )\n",
    "\n",
    "model.config = config\n",
    "model.bert = model.wrapped_model.bert  # Directly assigning the bert submodule\n",
    "model.cls = model.wrapped_model.cls \n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    if any(target_module in name and \"base_layer\" in name for target_module in target_modules):\n",
    "        param.requires_grad = False\n",
    "        #print(f\"{name}: gradients disabled (Base layer with LoRA alternative)\")\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "        #print(f\"{name}: gradients enabled\")\n",
    "#check_requires_grad(model)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"model_BERT\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    #save_steps=50,\n",
    "    eval_steps=3500,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate= 5e-4,\n",
    "    weight_decay=0.02,\n",
    "    logging_steps=5,\n",
    "    num_train_epochs=1,\n",
    "    save_strategy  = \"no\",\n",
    "    warmup_ratio = 0.15,\n",
    "    bf16 = True,\n",
    "    report_to=\"wandb\",\n",
    "    seed = 42\n",
    "    #max_steps = 1000\n",
    "    #load_best_model_at_end=True,  # Load the best model at the end\n",
    "    #metric_for_best_model=\"eval_f1_macro\",   # Metric to compare for best model\n",
    "    #greater_is_better=True\n",
    "\n",
    "\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data1,\n",
    "    eval_dataset=validation_data1,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    ignore_keys_for_eval=[\"bert\"],\n",
    "\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf3f5290-5860-4ab9-9753-56f0ec29d0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-21 19:44:34.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:34.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelyanhristov06\u001b[0m (\u001b[33mdelyanhristov06-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/delyanh/Projects/ReVeRA/ReLoRa/wandb/run-20240921_194435-k24va5m7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeachy-breeze-728\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/k24va5m7\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtraining_config                None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_config                   None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_name_or_path             bert_c4\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmodel_revision                 None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmed_up_model                None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mresume_from                    None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mload_optimizer_state_on_resume True\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdataset_path                   /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmegatron_dataset_config        None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_length                     512\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mbatch_size                     64\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mgradient_accumulation          1\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtotal_batch_size               64\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_peft                       True\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_r                         64\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlora_alpha                     128.0\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrelora                         15625\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtrain_scaling                  False\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mreset_optimizer_on_relora      True\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_random_pruning       0.0\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer_magnitude_pruning    0.0\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mforce_keep_original            False\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1moptimizer                      Adam\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mlr                             0.001\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mscheduler                      cosine_restarts\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcycle_length                   15625\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrestart_warmup_steps           500\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madjust_step                    0\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmin_lr_ratio                   0.1\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta1                     0.9\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1madam_beta2                     0.999\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mweight_decay                   0.02\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwarmup_steps                   9000\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mclip_grad_norm                 1.0\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1meval_every                     3500\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mnum_training_steps             62500\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_every                     600000\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1msave_dir                       checkpoint/peachy-breeze-728\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mkeep_checkpoints               None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mtags                           ['relora_bert_medium']\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mworkers                        4\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mquantize                       None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1muse_double_quant               False\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mdistributed_type               ddp\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mprofile                        False\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mautoresume                     False\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mcomment                        None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mwandb_watch                    False\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mskip_batches                   set()\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mseed                           42\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mrun_name                       peachy-breeze-728\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mLoading Huggingface dataset from directory\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mApplying set_format\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mChecking datasets size\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m473\u001b[0m - \u001b[1mLoading dataset preprocessing args to check on seq_length\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:37.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mAll good! Loading tokenizer now\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mTokenizer loaded\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1mUsing HuggingFace model bert_c4 revision None\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m565\u001b[0m - \u001b[1mWrapping model with LoRA (need_linear_weight=True)\u001b[0m\n",
      "trainable params: 36685626 || all params: 42977082 || trainable%: 85.36\n",
      "\u001b[32m2024-09-21 19:44:38.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m630\u001b[0m - \u001b[1m\n",
      "ReLoRaModel(\n",
      "  (wrapped_model): BertForMaskedLM(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 512)\n",
      "        (token_type_embeddings): Embedding(1, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-7): 8 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (key): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (value): ReLoRaLinear(\n",
      "                  (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): GELUActivation()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mTotal params  before LoRA: 41.40M\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m632\u001b[0m - \u001b[1mTotal params  after  LoRA: 42.98M\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mTrainable params: 36.69M\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m634\u001b[0m - \u001b[1mIn total, added 1.57M parameters to the model\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m636\u001b[0m - \u001b[1mSaving model to checkpoint/peachy-breeze-728 every 600000 update steps\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m657\u001b[0m - \u001b[1mWrapping model with DDP\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[32m2024-09-21 19:44:38.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mUsing Adam optimizer\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m721\u001b[0m - \u001b[1mScheduler will run for 62500 update steps\u001b[0m\n",
      "dido test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-21 19:44:38.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1mFull training set size: 4000000\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m763\u001b[0m - \u001b[1mDataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-09-21 19:44:38.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[1mTrain set size after shard: 4000000\u001b[0m\n",
      "dido2 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-21 19:44:38.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m772\u001b[0m - \u001b[1mSkipping the first 0 batches\u001b[0m\n",
      "Converting LoRA parameters to float32...\n",
      "Converted module.wrapped_model.bert.encoder.layer.0.attention.self.query.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.0.attention.self.query.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.0.attention.self.key.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.0.attention.self.key.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.0.attention.self.value.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.0.attention.self.value.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.1.attention.self.query.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.1.attention.self.query.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.1.attention.self.key.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.1.attention.self.key.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.1.attention.self.value.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.1.attention.self.value.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.2.attention.self.query.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.2.attention.self.query.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.2.attention.self.key.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.2.attention.self.key.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.2.attention.self.value.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.2.attention.self.value.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.3.attention.self.query.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.3.attention.self.query.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.3.attention.self.key.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.3.attention.self.key.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.3.attention.self.value.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.3.attention.self.value.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.4.attention.self.query.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.4.attention.self.query.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.4.attention.self.key.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.4.attention.self.key.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.4.attention.self.value.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.4.attention.self.value.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.5.attention.self.query.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.5.attention.self.query.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.5.attention.self.key.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.5.attention.self.key.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.5.attention.self.value.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.5.attention.self.value.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.6.attention.self.query.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.6.attention.self.query.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.6.attention.self.key.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.6.attention.self.key.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.6.attention.self.value.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.6.attention.self.value.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.7.attention.self.query.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.7.attention.self.query.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.7.attention.self.key.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.7.attention.self.key.lora_B to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.7.attention.self.value.lora_A to float32.\n",
      "Converted module.wrapped_model.bert.encoder.layer.7.attention.self.value.lora_B to float32.\n",
      "\u001b[32m2024-09-21 19:44:38.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m879\u001b[0m - \u001b[1mStarting training at update step 0 with 62500 update steps\u001b[0m\n",
      "Update steps:   0%|                                   | 0/62500 [00:00<?, ?it/s]dido3 test Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4000000\n",
      "})\n",
      "\u001b[32m2024-09-21 19:44:55.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m912\u001b[0m - \u001b[1mStarting first step\u001b[0m\n",
      "[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Update steps:   0%|                       | 1/62500 [00:16<288:46:22, 16.63s/it]Traceback (most recent call last):\n",
      "  File \"/home/delyanh/Projects/ReVeRA/ReLoRa/torchrun_main.py\", line 1168, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delyanh/Projects/ReVeRA/ReLoRa/torchrun_main.py\", line 926, in main\n",
      "    loss =  model(**batch).loss\n",
      "            ^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1593, in forward\n",
      "    else self._run_ddp_forward(*inputs, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1411, in _run_ddp_forward\n",
      "    return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1589, in forward\n",
      "    inputs, kwargs = self._pre_forward(*inputs, **kwargs)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1480, in _pre_forward\n",
      "    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \n",
      "making sure all `forward` function outputs participate in calculating loss. \n",
      "If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n",
      "Parameter indices which did not receive grad for rank 0: 6 7 9 10 12 13 25 26 28 29 31 32 44 45 47 48 50 51 63 64 66 67 69 70 82 83 85 86 88 89 101 102 104 105 107 108 120 121 123 124 126 127 139 140 142 143 145 146\n",
      " In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/delyanh/Projects/ReVeRA/ReLoRa/torchrun_main.py\", line 1168, in <module>\n",
      "[rank0]:     main(args)\n",
      "[rank0]:   File \"/home/delyanh/Projects/ReVeRA/ReLoRa/torchrun_main.py\", line 926, in main\n",
      "[rank0]:     loss =  model(**batch).loss\n",
      "[rank0]:             ^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1593, in forward\n",
      "[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)\n",
      "[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1411, in _run_ddp_forward\n",
      "[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1589, in forward\n",
      "[rank0]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)\n",
      "[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/media/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1480, in _pre_forward\n",
      "[rank0]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\n",
      "[rank0]:                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \n",
      "[rank0]: making sure all `forward` function outputs participate in calculating loss. \n",
      "[rank0]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n",
      "[rank0]: Parameter indices which did not receive grad for rank 0: 6 7 9 10 12 13 25 26 28 29 31 32 44 45 47 48 50 51 63 64 66 67 69 70 82 83 85 86 88 89 101 102 104 105 107 108 120 121 123 124 126 127 139 140 142 143 145 146\n",
      "[rank0]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.103 MB of 0.103 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           grad_norm 3.04401\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                loss 10.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     n_lora_restarts 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  n_optimizer_resets 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  throughput_batches 0.05995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: throughput_examples 3.83672\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   throughput_tokens 1523.1788\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         tokens_seen 25408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         update_step 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mpeachy-breeze-728\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining/runs/k24va5m7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/delyanhristov06-/peft_pretraining\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240921_194435-k24va5m7/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "E0921 19:45:07.726000 140214871113792 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 185603) of binary: /media/.venv/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/.venv/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 879, in main\n",
      "    run(args)\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 870, in run\n",
      "    elastic_launch(\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/media/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "torchrun_main.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-09-21_19:45:07\n",
      "  host      : radicho\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 185603)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# earthy-smoke-669\n",
    "!torchrun --nproc_per_node=1 torchrun_main.py \\\n",
    "    --model_name_or_path bert_c4 \\\n",
    "    --dataset_path /home/delyanh/Projects/ReVeRA/preprocessed_cola_dataset_BERT_MEDIUM \\\n",
    "    --batch_size 64\\\n",
    "    --lr 1e-3\\\n",
    "    --max_length 512 \\\n",
    "    --weight_decay 0.02\\\n",
    "    --num_training_steps 62500 \\\n",
    "    --save_every 600000\\\n",
    "    --eval_every 3500 \\\n",
    "    --warmup_steps 9000 \\\n",
    "    --seed 42\\\n",
    "    --tags relora_bert_medium \\\n",
    "    --workers 4 \\\n",
    "    --use_double_quant False \\\n",
    "    --use_peft True \\\n",
    "    --cycle_length 15625 \\\n",
    "    --restart_warmup_steps 500 \\\n",
    "    --scheduler cosine_restarts \\\n",
    "    --reset_optimizer_on_relora True \\\n",
    "    --relora 15625\\\n",
    "    --lora_r 64\\\n",
    "    --lora_alpha 128\\\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e53fe-d137-46e2-a38f-2b446f19fb5e",
   "metadata": {},
   "source": [
    "warm_up_reset 300\n",
    "warm_up 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7053e94b-18f3-4e0d-bd0c-011f0a00cdb9",
   "metadata": {},
   "source": [
    "compare with standart lora to see if everything is working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fdb885-282f-41c1-b605-152af8a42208",
   "metadata": {},
   "source": [
    "after that test fewer warm_up steps\n",
    "different magnitude pruning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
