{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/shared_home/vlialin/miniconda3/envs/peft_pretraining_shala/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import bitsandbytes.functional as bnbF\n",
    "\n",
    "from peft_pretraining.modeling_llama import LlamaForCausalLM\n",
    "from peft_pretraining.relora import ReLoRaModel\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/shared_home/vlialin/miniconda3/envs/peft_pretraining_shala/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "orig_model = LlamaForCausalLM.from_pretrained(\"../checkpoints/llama_250m-2023-06-09-11-29-56_up_to_5K/model_5000\")#, load_in_8bit=True)#, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"Why am I doing this?\", return_tensors=\"pt\").input_ids\n",
    "# orig_out = orig_model(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReLoRaModel(\n",
    "    orig_model,\n",
    "    r=128,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"attn\", \"attention\", \"mlp\"],\n",
    "    trainable_scaling=False,\n",
    "    keep_original_weights=True,\n",
    "    quantize4bit=True,\n",
    "    use_double_quant=True,\n",
    ")\n",
    "model = model.to(dtype=torch.bfloat16, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(trainable_parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids.cuda()\n",
    "quantized_out = model(input_ids, labels=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = quantized_out.loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = model.wrapped_model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_data_fp = bnb.functional.dequantize_4bit(weight.data, weight.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0106,  0.0106,  0.0656,  ..., -0.0438, -0.0330,  0.0354],\n",
       "        [ 0.0519,  0.0317,  0.0404,  ..., -0.0113,  0.0270, -0.0056],\n",
       "        [-0.0621,  0.0349, -0.0326,  ...,  0.0363,  0.0104,  0.0218],\n",
       "        ...,\n",
       "        [ 0.0286, -0.0145, -0.0267,  ...,  0.0047,  0.0199, -0.0309],\n",
       "        [-0.0207, -0.0048,  0.0231,  ...,  0.0368,  0.0368, -0.0186],\n",
       "        [-0.0327, -0.0246, -0.0057,  ..., -0.0520,  0.0293,  0.0000]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_data_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Params4bit([[153],\n",
       "            [247],\n",
       "            [114],\n",
       "            ...,\n",
       "            [198],\n",
       "            [ 48],\n",
       "            [215]], device='cuda:0', dtype=torch.uint8))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wrapped_model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLoRaModel(\n",
       "  (wrapped_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32000, 768, padding_idx=31999)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): ReLoRaLinear(\n",
       "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
       "              (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
       "            )\n",
       "            (k_proj): ReLoRaLinear(\n",
       "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
       "              (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
       "            )\n",
       "            (v_proj): ReLoRaLinear(\n",
       "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
       "              (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
       "            )\n",
       "            (o_proj): ReLoRaLinear(\n",
       "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
       "              (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
       "            )\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): ReLoRaLinear(\n",
       "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
       "              (lora_B): Linear(in_features=128, out_features=2560, bias=False)\n",
       "            )\n",
       "            (down_proj): ReLoRaLinear(\n",
       "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lora_A): Linear(in_features=2560, out_features=128, bias=False)\n",
       "              (lora_B): Linear(in_features=128, out_features=768, bias=False)\n",
       "            )\n",
       "            (up_proj): ReLoRaLinear(\n",
       "              (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lora_A): Linear(in_features=768, out_features=128, bias=False)\n",
       "              (lora_B): Linear(in_features=128, out_features=2560, bias=False)\n",
       "            )\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.1537,  1.7079,  0.5386,  ..., -3.2834, -3.8020, -4.5960],\n",
       "         [-3.6189,  3.0790,  1.9448,  ..., -3.3880, -4.8104, -4.4347],\n",
       "         [-3.0543,  2.4709,  0.8437,  ..., -3.1641, -4.6998, -4.3173],\n",
       "         ...,\n",
       "         [-4.4516,  1.1609,  1.1042,  ..., -5.4319, -4.2324, -5.0842],\n",
       "         [-4.5135,  5.6720,  2.0341,  ..., -1.5236, -4.5093, -4.6042],\n",
       "         [-4.4559,  0.2325,  2.2894,  ..., -2.6422, -5.2159, -4.2815]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.6308, device='cuda:0', grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(hf4bit_out.logits, orig_out.logits.cuda(), p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(83.9014, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(orig_out.logits.cpu(), quantized_out.logits.cpu(), p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug/bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 128\n",
    "out_features = 64\n",
    "use_double_quant = False\n",
    "\n",
    "weight = bnb.nn.Linear4bit(\n",
    "    in_features,\n",
    "    out_features,\n",
    "    bias=False,\n",
    "    compute_dtype=torch.bfloat16,\n",
    "    compress_statistics=use_double_quant,\n",
    "    quant_type=\"nf4\",\n",
    ")\n",
    "bias = torch.tensor(out_features, dtype=torch.bfloat16, requires_grad=True, device=\"cuda\")\n",
    "weight = weight.to(\"cuda\")\n",
    "\n",
    "lora_A = nn.Linear(in_features, 1, bias=False).to(\"cuda\", dtype=torch.bfloat16)\n",
    "lora_B = nn.Linear(1, out_features, bias=False).to(\"cuda\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, in_features, device=\"cuda\", dtype=torch.bfloat16)\n",
    "y = weight(x) + bias\n",
    "y = y + lora_B(lora_A(x))\n",
    "\n",
    "loss = y.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_weight = torch.randn(in_features, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(out_features, out_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight = bnb.nn.Params4bit(net[0].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_weight = bnb.nn.Params4bit(orig_weight.data, requires_grad=False, compress_statistics=False, quant_type=\"nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Params4bit([[ 0.5485, -0.2513,  0.2402,  ..., -0.7881, -0.4519, -1.0543],\n",
       "            [-0.3215, -0.1178, -0.0623,  ..., -0.2657, -0.2037,  3.4480],\n",
       "            [ 1.4118, -1.0065,  1.5193,  ..., -1.7599,  1.3230, -1.3040],\n",
       "            ...,\n",
       "            [ 1.5272,  1.4868,  0.7169,  ..., -0.0711, -0.4521,  0.9336],\n",
       "            [-0.0707, -1.3644,  1.0509,  ...,  0.7394, -1.6139, -0.9520],\n",
       "            [ 1.7725, -1.4115,  1.2637,  ...,  0.4864,  1.9556, -0.5330]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_pretraining_shala",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
