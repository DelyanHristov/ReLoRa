{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/shared_home/vlialin/miniconda3/envs/peft_pretraining_shala/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset c4 (/mnt/shared_home/hf_cache/datasets_cache/c4/realnewslike/0.0.0/df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01)\n",
      "100%|██████████| 2/2 [00:26<00:00, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"c4\", \"realnewslike\")\n",
    "print(\"2\")\n",
    "data_preprocessed = load_from_disk(\"../preprocessed_data/c4_realnewslike_t5-base_512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/shared_home/vlialin/miniconda3/envs/peft_pretraining_shala/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'timestamp', 'url'],\n",
       "        num_rows: 13799838\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'timestamp', 'url'],\n",
       "        num_rows: 13863\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['timestamp', 'url', 'input_ids'],\n",
       "        num_rows: 538176\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['timestamp', 'url', 'input_ids'],\n",
       "        num_rows: 528\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Netflix Inc said it added 8.\n",
      "84 million paid global streaming subscribers in the fourth quarter, while analysts had expected 9.\n",
      "18 million net global streaming additions.\n",
      " It was not immediately clear if analysts were excluding unpaid additions.\n",
      " Shares of the company were down 3 percent in after-hours trading.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_eol(text):\n",
    "    return \".\\n\".join(text.split('.'))\n",
    "\n",
    "_id = -1\n",
    "print()\n",
    "print(add_eol(data[\"train\"][_id][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIII News All-Freshman Team and didn't miss a game after breaking her nose in the middle of the schedule.\n",
      " Her nose lost the battle.\n",
      " Temple won the war.\n",
      " \"After the knee injuries, yeah, that wasn't really anything,\" Temple said.\n",
      " \"I just wore a mask for two weeks and sat out a couple practices and I was ready to go.\n",
      "\"</s></s> Courtney L.\n",
      " Blankenship, 25, Bucyrus, theft, fined $323, sentenced to 30 days in jail with all suspended.\n",
      " Chandler L.\n",
      " Lust, 18, Bucyrus, abuse of an intoxicant, fined $308, sentenced to 90 days in jail with all suspended; drug paraphernalia, sentenced to 30 days in jail with all suspended.\n",
      " Jessica K.\n",
      " Wade, 33, Bloomville, criminal damages, fined $325, sentenced to 30 days in jail with all suspended.\n",
      " Leslee D.\n",
      " Baxter, 31, Bucyrus, physical control, fined $1,000, sentenced to 90 days in jail with all suspended.\n",
      " Roger L.\n",
      " Boudinot, 65, Bucyrus, operating a vehicle under the influence, fined $625, sentenced to 30 days in jail with 27 suspended, driver’s license suspended for six months.\n",
      " Casey Taylor, 30, Galion, dog running at large, fined $180.\n",
      " Travis D.\n",
      " Lozier, 22, Galion, possession of a controlled substance, fined $155.\n",
      " Phillip M.\n",
      " Tesso Jr.\n",
      ", 24, Crestline, disorderly conduct, fined $155.\n",
      " Zachary A.\n",
      " Fout, 23, Galion, possession of marijuana, fined $155.\n",
      " Thelma J.\n",
      " Snyder, 78, Galion, violation of grade crossing, fined $150.\n",
      " Daniel E.\n",
      " Moore, 68, Bucyrus, violation of grade crossing, fined $150.\n",
      " Damon Schramek, 45, Galion, violation of grade crossing, fined $150.\n",
      " Natalie E.\n",
      " Davis, 37, Galion, driving under suspension, fined $280, sentenced to 30 days in jail with all suspended.\n",
      " David M.\n",
      " Corbett, 25, Crestline, driving under suspension, fined $230.\n",
      " Michael W.\n",
      " Conley, 39, Crestline, driving under suspension, fined $225.\n",
      " Daniel R.\n",
      " Smith, 37, Galion, driving under suspension, fined $225.\n",
      " Anisha\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# for _id in tqdm(range(538176)):\n",
    "_id = -1\n",
    "input_ids = data_preprocessed[\"train\"][_id][\"input_ids\"]\n",
    "# print(type(input_ids), len(input_ids))\n",
    "decoded = add_eol(tokenizer.decode(input_ids))\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_pretraining_shala",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
