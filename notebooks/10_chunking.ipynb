{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, get_worker_info\n",
    "from transformers import AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def tokenize_and_chunk(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    dataset: Dataset,\n",
    "    text_field: str,\n",
    "    sequence_length: int,\n",
    "    num_cpu: int = multiprocessing.cpu_count(),\n",
    "):\n",
    "    \"\"\"\n",
    "    Build data loaders for training.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Load the tokenizer from the pretrained \"EleutherAI/gpt-neox-20b\" model.\n",
    "    2. Load the \"openwebtext\" dataset.\n",
    "    3. Tokenize the dataset, adding the end-of-sentence token to each text.\n",
    "    4. Process the tokenized dataset into chunks of a specified block size.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: The processed dataset ready for training.\n",
    "    \"\"\"\n",
    "    extra_map_kwargs = {\"num_proc\": num_cpu}\n",
    "    if isinstance(dataset, IterableDataset):\n",
    "        extra_map_kwargs = {}\n",
    "\n",
    "    current_columns = dataset.column_names\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda example: tokenizer([t + tokenizer.eos_token for t in example[text_field]]),\n",
    "        batched=True,\n",
    "        remove_columns=current_columns,\n",
    "        **extra_map_kwargs,\n",
    "    )\n",
    "\n",
    "    block_size = sequence_length\n",
    "\n",
    "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    train_dataset = tokenized_dataset.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        **extra_map_kwargs,\n",
    "    )\n",
    "\n",
    "    return train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "import datasets\n",
    "max_length = 2048\n",
    "\n",
    "data = datasets.load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", model_max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tokenize_and_chunk(tokenizer, data, text_field=\"text\", sequence_length=max_length, num_cpu=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=2, collate_fn=default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3513881/2323961505.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(batch[\"input_ids\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[12847,   277, 15068,  ...,     8,   414,    13],\n",
       "        [  336,   471,     5,  ...,    28,    46,  1287],\n",
       "        [    6,  9445,  8424,  ...,    45,     8,   814],\n",
       "        ...,\n",
       "        [   21,     8,   471,  ...,   979,    16,   112],\n",
       "        [23659,   774,     5,  ...,    19,    92,    46],\n",
       "        [  256, 11577,   412,  ...,   112,   372,    28]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor(batch[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_pretraining_shala",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
