{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fsx/vlialin/relora/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/vlialin/relora/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM as HF_GPTNeoXForCausalLM\n",
    "# from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from peft_pretraining.modeling_pythia import GPTNeoXForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_ids = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flashattention_model = GPTNeoXForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flashattention_out = flashattention_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model = HF_GPTNeoXForCausalLM.from_pretrained(model_name)\n",
    "# orig_model = BetterTransformer.transform(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_out = orig_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  3.6091,  -9.3721,   8.7746,  ...,  -9.2669,  -9.2728,  -9.3494],\n",
       "         [  1.8715, -10.9088,   5.3397,  ..., -10.6625, -10.8846, -10.8984],\n",
       "         [  1.1809, -10.6320,   6.5040,  ..., -10.4274, -10.7300, -10.4196],\n",
       "         [  5.8298,  -8.4654,  12.3902,  ...,  -8.4833,  -8.6062,  -8.5475],\n",
       "         [  3.7810,  -9.6519,   6.0415,  ...,  -9.6365,  -9.8250,  -9.5622],\n",
       "         [  8.1836,  -8.4023,  14.3205,  ...,  -8.5621,  -8.4343,  -8.4242]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  3.6091,  -9.3721,   8.7746,  ...,  -9.2669,  -9.2728,  -9.3494],\n",
       "         [  1.8715, -10.9088,   5.3397,  ..., -10.6625, -10.8846, -10.8984],\n",
       "         [  1.1809, -10.6320,   6.5040,  ..., -10.4274, -10.7300, -10.4196],\n",
       "         [  5.8298,  -8.4654,  12.3902,  ...,  -8.4833,  -8.6062,  -8.5475],\n",
       "         [  3.7810,  -9.6519,   6.0415,  ...,  -9.6365,  -9.8250,  -9.5622],\n",
       "         [  8.1836,  -8.4023,  14.3205,  ...,  -8.5621,  -8.4343,  -8.4242]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flashattention_out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(orig_out.logits, flashattention_out.logits, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/mnt/shared_home/vlialin/miniconda3/envs/peft_pretraining_shala/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is cute, but he\\'s not a dog. He\\'s a cat.\"\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_gen = orig_model.generate(input_ids)\n",
    "tokenizer.decode(out_gen[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is cute, I am. I am. I am. I am. I'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_gen = flashattention_model.generate(input_ids)\n",
    "tokenizer.decode(out_gen[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_pretraining_shala",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
